{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SamapanThongmee/Daily_Deposit_Stock_Data/blob/main/Feed_to_GoogleSheet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ralok5CB93ZR"
      },
      "source": [
        "### **Install Packages and Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97LyITdN6CXA",
        "outputId": "f1b57038-fe3b-4150-8a75-18dace02d691"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actually_Starting:  2023-06-08 16:19:12.235486\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.10.0-py3-none-any.whl (6.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3[socks]<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.26.15)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.22.0-py3-none-any.whl (384 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m384.9/384.9 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.10.3-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2022.12.7)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (23.1.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Collecting async-generator>=1.9 (from trio~=0.17->selenium)\n",
            "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.4)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.1.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: outcome, h11, async-generator, wsproto, trio, trio-websocket, selenium\n",
            "Successfully installed async-generator-1.10 h11-0.14.0 outcome-1.2.0 selenium-4.10.0 trio-0.22.0 trio-websocket-0.10.3 wsproto-1.2.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting thaifin\n",
            "  Downloading thaifin-0.2.2-py3-none-any.whl (8.9 kB)\n",
            "Collecting arrow<0.17.0,>=0.16.0 (from thaifin)\n",
            "  Downloading arrow-0.16.0-py2.py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from thaifin) (4.11.2)\n",
            "Collecting cachetools<5.0.0,>=4.2.1 (from thaifin)\n",
            "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
            "Collecting furl<3.0.0,>=2.1.0 (from thaifin)\n",
            "  Downloading furl-2.1.3-py2.py3-none-any.whl (20 kB)\n",
            "Collecting fuzzywuzzy<0.19.0,>=0.18.0 (from thaifin)\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: lxml<5.0.0,>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from thaifin) (4.9.2)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from thaifin) (1.22.4)\n",
            "Requirement already satisfied: pandas<2.0.0,>=1.0.5 in /usr/local/lib/python3.10/dist-packages (from thaifin) (1.5.3)\n",
            "Requirement already satisfied: pydantic<2.0.0,>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from thaifin) (1.10.7)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /usr/local/lib/python3.10/dist-packages (from thaifin) (2.27.1)\n",
            "Collecting tenacity<7.0.0,>=6.3.1 (from thaifin)\n",
            "  Downloading tenacity-6.3.1-py2.py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from arrow<0.17.0,>=0.16.0->thaifin) (2.8.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->thaifin) (2.4.1)\n",
            "Requirement already satisfied: six>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from furl<3.0.0,>=2.1.0->thaifin) (1.16.0)\n",
            "Collecting orderedmultidict>=1.0.1 (from furl<3.0.0,>=2.1.0->thaifin)\n",
            "  Downloading orderedmultidict-1.0.1-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.0.0,>=1.0.5->thaifin) (2022.7.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.0.0,>=1.6.1->thaifin) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->thaifin) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->thaifin) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->thaifin) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->thaifin) (3.4)\n",
            "Installing collected packages: fuzzywuzzy, tenacity, orderedmultidict, cachetools, furl, arrow, thaifin\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 8.2.2\n",
            "    Uninstalling tenacity-8.2.2:\n",
            "      Successfully uninstalled tenacity-8.2.2\n",
            "  Attempting uninstall: cachetools\n",
            "    Found existing installation: cachetools 5.3.0\n",
            "    Uninstalling cachetools-5.3.0:\n",
            "      Successfully uninstalled cachetools-5.3.0\n",
            "Successfully installed arrow-0.16.0 cachetools-4.2.4 furl-2.1.3 fuzzywuzzy-0.18.0 orderedmultidict-1.0.1 tenacity-6.3.1 thaifin-0.2.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gspread in /usr/local/lib/python3.10/dist-packages (3.4.2)\n",
            "Requirement already satisfied: requests>=2.2.1 in /usr/local/lib/python3.10/dist-packages (from gspread) (2.27.1)\n",
            "Requirement already satisfied: google-auth in /usr/local/lib/python3.10/dist-packages (from gspread) (2.17.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.2.1->gspread) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.2.1->gspread) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.2.1->gspread) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.2.1->gspread) (3.4)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth->gspread) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth->gspread) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth->gspread) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth->gspread) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth->gspread) (0.5.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gspread_dataframe in /usr/local/lib/python3.10/dist-packages (3.0.8)\n",
            "Requirement already satisfied: gspread>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from gspread_dataframe) (3.4.2)\n",
            "Requirement already satisfied: pandas>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gspread_dataframe) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.2.1 in /usr/local/lib/python3.10/dist-packages (from gspread>=3.0.0->gspread_dataframe) (2.27.1)\n",
            "Requirement already satisfied: google-auth in /usr/local/lib/python3.10/dist-packages (from gspread>=3.0.0->gspread_dataframe) (2.17.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.14.0->gspread_dataframe) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.14.0->gspread_dataframe) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.14.0->gspread_dataframe) (1.22.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=0.14.0->gspread_dataframe) (1.16.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.2.1->gspread>=3.0.0->gspread_dataframe) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.2.1->gspread>=3.0.0->gspread_dataframe) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.2.1->gspread>=3.0.0->gspread_dataframe) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.2.1->gspread>=3.0.0->gspread_dataframe) (3.4)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth->gspread>=3.0.0->gspread_dataframe) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth->gspread>=3.0.0->gspread_dataframe) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth->gspread>=3.0.0->gspread_dataframe) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth->gspread>=3.0.0->gspread_dataframe) (0.5.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.10/dist-packages (4.1.3)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from oauth2client) (0.21.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client) (0.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from oauth2client) (0.3.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from oauth2client) (4.9)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from oauth2client) (1.16.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2>=0.9.1->oauth2client) (3.0.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
            "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
          ]
        }
      ],
      "source": [
        "import datetime as dt\n",
        "Actually_Starting = dt.datetime.now()\n",
        "print('Actually_Starting: ', Actually_Starting)\n",
        "\n",
        "!pip install selenium\n",
        "!pip install thaifin\n",
        "!pip install gspread\n",
        "!pip install gspread_dataframe\n",
        "!pip install oauth2client\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "from functools import reduce\n",
        "from thaifin import Stock\n",
        "import math\n",
        "from datetime import datetime, timedelta\n",
        "from pytz import timezone\n",
        "from selenium import webdriver\n",
        "import gspread\n",
        "import gspread_dataframe as gd\n",
        "from gspread_dataframe import get_as_dataframe, set_with_dataframe\n",
        "import os\n",
        "import json\n",
        "import requests\n",
        "from oauth2client.service_account import ServiceAccountCredentials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gu5TwN6Ht1VA",
        "outputId": "65409bdb-2058-46b7-ea1f-d9733238f51b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing: /tmp/apt-key-gpghome.Xt51HWOr9Y/gpg.1.sh --keyserver keyserver.ubuntu.com --recv-keys DCC9EFBF77E11517\n",
            "gpg: key DCC9EFBF77E11517: public key \"Debian Stable Release Key (10/buster) <debian-release@lists.debian.org>\" imported\n",
            "gpg: Total number processed: 1\n",
            "gpg:               imported: 1\n",
            "Executing: /tmp/apt-key-gpghome.4qwk2toDvh/gpg.1.sh --keyserver keyserver.ubuntu.com --recv-keys 648ACFD622F3D138\n",
            "gpg: key DC30D7C23CBBABEE: public key \"Debian Archive Automatic Signing Key (10/buster) <ftpmaster@debian.org>\" imported\n",
            "gpg: Total number processed: 1\n",
            "gpg:               imported: 1\n",
            "Executing: /tmp/apt-key-gpghome.kt3dWMZxRi/gpg.1.sh --keyserver keyserver.ubuntu.com --recv-keys 112695A0E562B32A\n",
            "gpg: key 4DFAB270CAA96DFA: public key \"Debian Security Archive Automatic Signing Key (10/buster) <ftpmaster@debian.org>\" imported\n",
            "gpg: Total number processed: 1\n",
            "gpg:               imported: 1\n",
            "Warning: apt-key output should not be parsed (stdout is not a terminal)\n",
            "Warning: apt-key output should not be parsed (stdout is not a terminal)\n",
            "Warning: apt-key output should not be parsed (stdout is not a terminal)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "%%shell\n",
        "\n",
        "# Add debian buster\n",
        "cat > /etc/apt/sources.list.d/debian.list <<'EOF'\n",
        "deb [arch=amd64 signed-by=/usr/share/keyrings/debian-buster.gpg] http://deb.debian.org/debian buster main\n",
        "deb [arch=amd64 signed-by=/usr/share/keyrings/debian-buster-updates.gpg] http://deb.debian.org/debian buster-updates main\n",
        "deb [arch=amd64 signed-by=/usr/share/keyrings/debian-security-buster.gpg] http://deb.debian.org/debian-security buster/updates main\n",
        "EOF\n",
        "\n",
        "# Add keys\n",
        "apt-key adv --keyserver keyserver.ubuntu.com --recv-keys DCC9EFBF77E11517\n",
        "apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 648ACFD622F3D138\n",
        "apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 112695A0E562B32A\n",
        "\n",
        "apt-key export 77E11517 | gpg --dearmour -o /usr/share/keyrings/debian-buster.gpg\n",
        "apt-key export 22F3D138 | gpg --dearmour -o /usr/share/keyrings/debian-buster-updates.gpg\n",
        "apt-key export E562B32A | gpg --dearmour -o /usr/share/keyrings/debian-security-buster.gpg\n",
        "\n",
        "# Prefer debian repo for chromium* packages only\n",
        "# Note the double-blank lines between entries\n",
        "cat > /etc/apt/preferences.d/chromium.pref << 'EOF'\n",
        "Package: *\n",
        "Pin: release a=eoan\n",
        "Pin-Priority: 500\n",
        "\n",
        "Package: *\n",
        "Pin: origin \"deb.debian.org\"\n",
        "Pin-Priority: 300\n",
        "\n",
        "Package: chromium*\n",
        "Pin: origin \"deb.debian.org\"\n",
        "Pin-Priority: 700\n",
        "EOF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnJTIyZ0t2fa",
        "outputId": "93e66e38-5ed8-4ee3-a880-d44376bdc58d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://deb.debian.org/debian buster InRelease [122 kB]\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.39)] [Connecting to security.\r0% [Connecting to archive.ubuntu.com (185.125.190.39)] [Connecting to security.\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "\r                                                                               \rHit:3 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n",
            "\r                                                                               \rGet:4 http://deb.debian.org/debian buster-updates InRelease [56.6 kB]\n",
            "\r                                                                               \rGet:5 http://deb.debian.org/debian-security buster/updates InRelease [34.8 kB]\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (91.189.91.38)] [Co\r                                                                               \rGet:6 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
            "\r0% [6 InRelease 12.7 kB/114 kB 11%] [Connecting to security.ubuntu.com (91.189.\r                                                                               \rGet:7 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B]\n",
            "\r0% [6 InRelease 15.6 kB/114 kB 14%] [Connecting to security.ubuntu.com (91.189.\r0% [6 InRelease 15.6 kB/114 kB 14%] [Connecting to security.ubuntu.com (91.189.\r                                                                               \rHit:8 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "\r0% [6 InRelease 15.6 kB/114 kB 14%] [Connecting to security.ubuntu.com (91.189.\r                                                                               \rGet:9 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease [18.1 kB]\n",
            "\r0% [6 InRelease 90.1 kB/114 kB 79%] [Connecting to security.ubuntu.com (91.189.\r0% [Waiting for headers] [Connecting to security.ubuntu.com (91.189.91.38)] [9 \r                                                                               \rHit:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (91.189.91.38)] [9 \r0% [Waiting for headers] [Waiting for headers] [Connecting to ppa.launchpad.net\r                                                                               \rGet:11 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
            "\r0% [11 InRelease 6,947 B/108 kB 6%] [Waiting for headers] [Connecting to ppa.la\r                                                                               \r0% [Waiting for headers] [Waiting for headers]\r                                              \rHit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "\r0% [Waiting for headers] [Connecting to ppa.launchpad.net (185.125.190.52)]\r0% [Waiting for headers] [Connecting to ppa.launchpad.net (185.125.190.52)]\r                                                                           \rGet:13 http://deb.debian.org/debian buster/main amd64 Packages [10.7 MB]\n",
            "\r0% [Waiting for headers] [Waiting for headers] [13 Packages 65.0 kB/10.7 MB 1%]\r                                                                               \rHit:14 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Get:15 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
            "Get:16 http://deb.debian.org/debian buster-updates/main amd64 Packages [9,745 B]\n",
            "Get:17 http://deb.debian.org/debian-security buster/updates/main amd64 Packages [648 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1,354 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3,255 kB]\n",
            "Get:20 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 Packages [29.6 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2,774 kB]\n",
            "Get:22 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1,058 kB]\n",
            "Fetched 20.4 MB in 3s (7,780 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  chromium-common chromium-sandbox libevent-2.1-6 libfontenc1 libicu63\n",
            "  libimobiledevice6 libjpeg62-turbo libplist3 libre2-5 libu2f-udev libudev1\n",
            "  libupower-glib3 libusbmuxd6 libvpx5 libxkbfile1 libxtst6 libxxf86dga1\n",
            "  notification-daemon udev upower usbmuxd x11-utils\n",
            "Suggested packages:\n",
            "  chromium-l10n chromium-shell libusbmuxd-tools mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  chromium chromium-common chromium-driver chromium-sandbox libevent-2.1-6\n",
            "  libfontenc1 libicu63 libimobiledevice6 libjpeg62-turbo libplist3 libre2-5\n",
            "  libu2f-udev libupower-glib3 libusbmuxd6 libvpx5 libxkbfile1 libxtst6\n",
            "  libxxf86dga1 notification-daemon udev upower usbmuxd x11-utils\n",
            "The following packages will be upgraded:\n",
            "  libudev1\n",
            "1 upgraded, 23 newly installed, 0 to remove and 47 not upgraded.\n",
            "Need to get 76.2 MB of archives.\n",
            "After this operation, 266 MB of additional disk space will be used.\n",
            "Get:1 http://deb.debian.org/debian buster/main amd64 libevent-2.1-6 amd64 2.1.8-stable-4 [177 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libudev1 amd64 245.4-4ubuntu3.21 [75.9 kB]\n",
            "Get:3 http://deb.debian.org/debian buster/main amd64 libicu63 amd64 63.1-6+deb10u3 [8,293 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 udev amd64 245.4-4ubuntu3.21 [1,366 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu focal/main amd64 libre2-5 amd64 20200101+dfsg-1build1 [162 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu focal/main amd64 libfontenc1 amd64 1:1.1.4-0ubuntu1 [14.0 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu focal/main amd64 libxkbfile1 amd64 1:1.1.0-1 [65.3 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu focal/main amd64 libxtst6 amd64 2:1.2.3-1 [12.8 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu focal/main amd64 libxxf86dga1 amd64 2:1.1.5-0ubuntu1 [12.0 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu focal/main amd64 x11-utils amd64 7.7+5 [199 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu focal/main amd64 libplist3 amd64 2.1.0-4build2 [31.6 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu focal/main amd64 libusbmuxd6 amd64 2.0.1-2 [19.1 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu focal/main amd64 libimobiledevice6 amd64 1.2.1~git20191129.9f79242-1build1 [65.2 kB]\n",
            "Get:14 http://deb.debian.org/debian buster/main amd64 libjpeg62-turbo amd64 1:1.5.2-2+deb10u1 [133 kB]\n",
            "Get:15 http://deb.debian.org/debian buster/main amd64 libvpx5 amd64 1.7.0-3+deb10u1 [800 kB]\n",
            "Get:16 http://deb.debian.org/debian buster/main amd64 chromium-common amd64 90.0.4430.212-1~deb10u1 [1,423 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu focal/main amd64 libu2f-udev all 1.1.10-1 [6,108 B]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu focal/main amd64 libupower-glib3 amd64 0.99.11-1build2 [43.2 kB]\n",
            "Get:19 http://deb.debian.org/debian buster/main amd64 chromium amd64 90.0.4430.212-1~deb10u1 [58.3 MB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu focal/universe amd64 notification-daemon amd64 3.20.0-4 [37.0 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu focal/main amd64 upower amd64 0.99.11-1build2 [104 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu focal/main amd64 usbmuxd amd64 1.1.1~git20191130.9af2b12-1 [38.4 kB]\n",
            "Get:23 http://deb.debian.org/debian buster/main amd64 chromium-driver amd64 90.0.4430.212-1~deb10u1 [4,703 kB]\n",
            "Get:24 http://deb.debian.org/debian buster/main amd64 chromium-sandbox amd64 90.0.4430.212-1~deb10u1 [146 kB]\n",
            "Fetched 76.2 MB in 1s (74.1 MB/s)\n",
            "(Reading database ... 122541 files and directories currently installed.)\n",
            "Preparing to unpack .../libudev1_245.4-4ubuntu3.21_amd64.deb ...\n",
            "Unpacking libudev1:amd64 (245.4-4ubuntu3.21) over (245.4-4ubuntu3.19) ...\n",
            "Setting up libudev1:amd64 (245.4-4ubuntu3.21) ...\n",
            "Selecting previously unselected package udev.\n",
            "(Reading database ... 122541 files and directories currently installed.)\n",
            "Preparing to unpack .../00-udev_245.4-4ubuntu3.21_amd64.deb ...\n",
            "Unpacking udev (245.4-4ubuntu3.21) ...\n",
            "Selecting previously unselected package libevent-2.1-6:amd64.\n",
            "Preparing to unpack .../01-libevent-2.1-6_2.1.8-stable-4_amd64.deb ...\n",
            "Unpacking libevent-2.1-6:amd64 (2.1.8-stable-4) ...\n",
            "Selecting previously unselected package libicu63:amd64.\n",
            "Preparing to unpack .../02-libicu63_63.1-6+deb10u3_amd64.deb ...\n",
            "Unpacking libicu63:amd64 (63.1-6+deb10u3) ...\n",
            "Selecting previously unselected package libjpeg62-turbo:amd64.\n",
            "Preparing to unpack .../03-libjpeg62-turbo_1%3a1.5.2-2+deb10u1_amd64.deb ...\n",
            "Unpacking libjpeg62-turbo:amd64 (1:1.5.2-2+deb10u1) ...\n",
            "Selecting previously unselected package libre2-5:amd64.\n",
            "Preparing to unpack .../04-libre2-5_20200101+dfsg-1build1_amd64.deb ...\n",
            "Unpacking libre2-5:amd64 (20200101+dfsg-1build1) ...\n",
            "Selecting previously unselected package libvpx5:amd64.\n",
            "Preparing to unpack .../05-libvpx5_1.7.0-3+deb10u1_amd64.deb ...\n",
            "Unpacking libvpx5:amd64 (1.7.0-3+deb10u1) ...\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "Preparing to unpack .../06-libfontenc1_1%3a1.1.4-0ubuntu1_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-0ubuntu1) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../07-libxkbfile1_1%3a1.1.0-1_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1) ...\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "Preparing to unpack .../08-libxtst6_2%3a1.2.3-1_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1) ...\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "Preparing to unpack .../09-libxxf86dga1_2%3a1.1.5-0ubuntu1_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu1) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../10-x11-utils_7.7+5_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+5) ...\n",
            "Selecting previously unselected package chromium-common.\n",
            "Preparing to unpack .../11-chromium-common_90.0.4430.212-1~deb10u1_amd64.deb ...\n",
            "Unpacking chromium-common (90.0.4430.212-1~deb10u1) ...\n",
            "Selecting previously unselected package chromium.\n",
            "Preparing to unpack .../12-chromium_90.0.4430.212-1~deb10u1_amd64.deb ...\n",
            "Unpacking chromium (90.0.4430.212-1~deb10u1) ...\n",
            "Selecting previously unselected package chromium-driver.\n",
            "Preparing to unpack .../13-chromium-driver_90.0.4430.212-1~deb10u1_amd64.deb ...\n",
            "Unpacking chromium-driver (90.0.4430.212-1~deb10u1) ...\n",
            "Selecting previously unselected package chromium-sandbox.\n",
            "Preparing to unpack .../14-chromium-sandbox_90.0.4430.212-1~deb10u1_amd64.deb ...\n",
            "Unpacking chromium-sandbox (90.0.4430.212-1~deb10u1) ...\n",
            "Selecting previously unselected package libplist3:amd64.\n",
            "Preparing to unpack .../15-libplist3_2.1.0-4build2_amd64.deb ...\n",
            "Unpacking libplist3:amd64 (2.1.0-4build2) ...\n",
            "Selecting previously unselected package libusbmuxd6:amd64.\n",
            "Preparing to unpack .../16-libusbmuxd6_2.0.1-2_amd64.deb ...\n",
            "Unpacking libusbmuxd6:amd64 (2.0.1-2) ...\n",
            "Selecting previously unselected package libimobiledevice6:amd64.\n",
            "Preparing to unpack .../17-libimobiledevice6_1.2.1~git20191129.9f79242-1build1_amd64.deb ...\n",
            "Unpacking libimobiledevice6:amd64 (1.2.1~git20191129.9f79242-1build1) ...\n",
            "Selecting previously unselected package libu2f-udev.\n",
            "Preparing to unpack .../18-libu2f-udev_1.1.10-1_all.deb ...\n",
            "Unpacking libu2f-udev (1.1.10-1) ...\n",
            "Selecting previously unselected package libupower-glib3:amd64.\n",
            "Preparing to unpack .../19-libupower-glib3_0.99.11-1build2_amd64.deb ...\n",
            "Unpacking libupower-glib3:amd64 (0.99.11-1build2) ...\n",
            "Selecting previously unselected package notification-daemon.\n",
            "Preparing to unpack .../20-notification-daemon_3.20.0-4_amd64.deb ...\n",
            "Unpacking notification-daemon (3.20.0-4) ...\n",
            "Selecting previously unselected package upower.\n",
            "Preparing to unpack .../21-upower_0.99.11-1build2_amd64.deb ...\n",
            "Unpacking upower (0.99.11-1build2) ...\n",
            "Selecting previously unselected package usbmuxd.\n",
            "Preparing to unpack .../22-usbmuxd_1.1.1~git20191130.9af2b12-1_amd64.deb ...\n",
            "Unpacking usbmuxd (1.1.1~git20191130.9af2b12-1) ...\n",
            "Setting up libplist3:amd64 (2.1.0-4build2) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu1) ...\n",
            "Setting up chromium-sandbox (90.0.4430.212-1~deb10u1) ...\n",
            "Setting up libicu63:amd64 (63.1-6+deb10u3) ...\n",
            "Setting up notification-daemon (3.20.0-4) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-0ubuntu1) ...\n",
            "Setting up libjpeg62-turbo:amd64 (1:1.5.2-2+deb10u1) ...\n",
            "Setting up udev (245.4-4ubuntu3.21) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up libevent-2.1-6:amd64 (2.1.8-stable-4) ...\n",
            "Setting up libusbmuxd6:amd64 (2.0.1-2) ...\n",
            "Setting up libupower-glib3:amd64 (0.99.11-1build2) ...\n",
            "Setting up libre2-5:amd64 (20200101+dfsg-1build1) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1) ...\n",
            "Setting up libimobiledevice6:amd64 (1.2.1~git20191129.9f79242-1build1) ...\n",
            "Setting up libvpx5:amd64 (1.7.0-3+deb10u1) ...\n",
            "Setting up libu2f-udev (1.1.10-1) ...\n",
            "Failed to send reload request: No such file or directory\n",
            "Setting up upower (0.99.11-1build2) ...\n",
            "Setting up usbmuxd (1.1.1~git20191130.9af2b12-1) ...\n",
            "Warning: The home dir /var/lib/usbmux you specified can't be accessed: No such file or directory\n",
            "Adding system user `usbmux' (UID 105) ...\n",
            "Adding new user `usbmux' (UID 105) with group `plugdev' ...\n",
            "Not creating home directory `/var/lib/usbmux'.\n",
            "Setting up x11-utils (7.7+5) ...\n",
            "Setting up chromium-common (90.0.4430.212-1~deb10u1) ...\n",
            "Setting up chromium (90.0.4430.212-1~deb10u1) ...\n",
            "update-alternatives: using /usr/bin/chromium to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-driver (90.0.4430.212-1~deb10u1) ...\n",
            "Processing triggers for systemd (245.4-4ubuntu3.21) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Processing triggers for dbus (1.12.16-2ubuntu2.3) ...\n",
            "Processing triggers for mime-support (3.64ubuntu1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n"
          ]
        }
      ],
      "source": [
        "!apt-get update\n",
        "!apt-get install chromium chromium-driver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2XdprBXHt4UK"
      },
      "outputs": [],
      "source": [
        "from selenium import webdriver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nbAr_aM3t6FA"
      },
      "outputs": [],
      "source": [
        "def web_driver():\n",
        "    options = webdriver.ChromeOptions()\n",
        "    options.add_argument(\"--verbose\")\n",
        "    options.add_argument('--no-sandbox')\n",
        "    options.add_argument('--headless')\n",
        "    options.add_argument('--disable-gpu')\n",
        "    options.add_argument(\"--window-size=1920, 1200\")\n",
        "    options.add_argument('--disable-dev-shm-usage')\n",
        "    driver = webdriver.Chrome(options=options)\n",
        "    return driver"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZReWjW2jIlTG"
      },
      "source": [
        "### **JSON File**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "e7D2FMP0IkLU"
      },
      "outputs": [],
      "source": [
        "# JSON file\n",
        "scopes = ['https://www.googleapis.com/auth/spreadsheets', 'https://www.googleapis.com/auth/drive']\n",
        "secret_key = {\n",
        "  \"type\": \"service_account\",\n",
        "  \"project_id\": \"authentic-bongo-379415\",\n",
        "  \"private_key_id\": \"405452f0ce0216d5549d7316fe776cd55f92a7e6\",\n",
        "  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nMIIEvwIBADANBgkqhkiG9w0BAQEFAASCBKkwggSlAgEAAoIBAQC/quBOgNCgdJI5\\nJ3IWJCKRZB4GURhzv5mhE1bOU44TQ4D54ZwnYc4mpyO5AdVStTiAbl+DEHsqrdKe\\ndS38zyffxwQPXouM/36cDpRWGM/m00BGJ4b6iMp3sIY5jK0oQXjQXUNQdVmdIvaS\\noAHge+lPWP6U1lCyklR5ctkeg9upEKq346U1ON/gl/ie+j8bTfCNsrulqHTQVcZH\\n3tvY9JFlgHF0J3je9mJ59Aq4fE+totYd8UZE5Vd84eYGcDmjUh4SeCZJWg3rBHH4\\nTujPYqEZ/qzz01OtjUPAY4xJhd7FrQN8mThikKkdylayjqDoy8eHBa2bpdFQDQsC\\nGRAFqAm5AgMBAAECggEATDYayLxJuHOBrP41S/6ETEF9+s8GJQg7gyPYcWQh6yD9\\nSN7fo30tZosxFiBXfXKXi+HXUpscDmVdiSpY5dxNI5rTz3StkpwI8O6PJwjoqgS3\\n2u1eq4H21862IG4CcC5QZrGe0YBH6MZ9OuIqvOMoayec2vu4zLiIpF35XTva/Qb2\\nYCWSfKW1H2NoBzp5UDOLcH/erni94ZTridYHvcoLfhOllqhFFXVKbaAhbNiycAwo\\nummI99TnkXQbcEX/FDwtD6VwgQhYLhzFTgsAvPpfObm8sg7y3gE1RFvk91nGR9d5\\nSCwF7/+y1MycPc8xRwoFyhVavzyoou/onMKr9SvdjQKBgQDfzzvnFs0CbWVpRkgO\\nQ0xVdjfxWGrCghmCPHpCmEKCVgOCp3uHI7LdjCHCZvcdAwTP77clG/ar32EbUEQm\\nA9WSIwWLFw4zzj2yIRe5J8HmySGx9LxhF/R4e5qfbI3JYCBtdDOOj4lrAcyU0f3i\\nwRPAU1QarRvSxTauwzVvR28/YwKBgQDbPCiYlGdtSj8XUnZ53iNEEk8rpyALbKvz\\nvKElskf32vfhuJBIZnhm94+qmLjyfVza14dmeTROy79IUNWzqUdE14CD7c5KiD10\\n8gWY8ncGBICw4yJpq28/CY4i1UKITOJEwO9zp8SAP544Sjd9/kTf4D+UXDpxIb8D\\ncPLjGtLDMwKBgQCl5PRgDJSVkgUzjXbnq5avWluJN6Ka3tiNosp1Bmt2NM+RTxfP\\nSfTRJ4f2k1/kT1KH1wYIKbwkFgXiuxJoozrxgaggO0y5tcwhd0ogYonlyXyCbggc\\naCc8D1LDjTFj7S7cMt/schTIDqNYRHdOVxOO4bVbuKkBCFHKJMQ84dRiRQKBgQCP\\n3FkTTobT+52M80rVS4Q+vfj/vypS9NudFFF2iBebuC9jChZ7u+oUPy0iGBd83/CX\\nlki+YJiHgEGj9Y9V1qXysaseZ2UwGViirdIofCrIYxoDePhy+NEGP4Hqr6B6CmGM\\nFkLK1PMn6Rcs/t6NKdYORnvSIfcjl65dVre7eTWGawKBgQDFj493vGU28PBtLWEj\\n+Bk/DspFGwvNAbcnlEgsf4r8u8ki4bIVs9xq0bu+KSXwVAoDIimP7uwDs2o5oJjl\\n9DPeXpHc4QBLDGmQNaTAaz2UPW+/4RMBeTPmAKV5FvbgSZf6TIDr3SfU1i5irKMu\\niJ5aXeBSvXXEC0og/JHG6GrmWQ==\\n-----END PRIVATE KEY-----\\n\",\n",
        "  \"client_email\": \"accessgooglesheet@authentic-bongo-379415.iam.gserviceaccount.com\",\n",
        "  \"client_id\": \"102815717104123074637\",\n",
        "  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n",
        "  \"token_uri\": \"https://oauth2.googleapis.com/token\",\n",
        "  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n",
        "  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/accessgooglesheet%40authentic-bongo-379415.iam.gserviceaccount.com\"\n",
        "}\n",
        "\n",
        "with open(\"secret_key.json\", \"w\") as outfile:\n",
        "    json.dump(secret_key, outfile)\n",
        "\n",
        "creds = ServiceAccountCredentials.from_json_keyfile_name('/content/secret_key.json', scopes=scopes)\n",
        "file = gspread.authorize(creds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbJwbcIugtRs"
      },
      "source": [
        "### **Prepare Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzgvmoFqtiQi"
      },
      "source": [
        "#### **SET Index**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KyYwHk5tlFx",
        "outputId": "818bd811-ef78-4f78-ebb2-c412b1d47701"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SET Finished\n"
          ]
        }
      ],
      "source": [
        "# SET\n",
        "driver = web_driver()\n",
        "driver.get('https://www.investing.com/indices/thailand-set-historical-data')\n",
        "data = driver.page_source\n",
        "data_df = pd.read_html(data)[1]\n",
        "data_df['Date'] = pd.to_datetime(data_df['Date'])\n",
        "data_df = (data_df.rename(columns={'Price':'Close'})[['Date', 'Open', 'High', 'Low', 'Close']]\n",
        "                  .sort_values('Date', ascending=True)\n",
        "                  .reset_index(drop=True))\n",
        "data_df['Date'] = pd.to_datetime(data_df['Date']).dt.date\n",
        "\n",
        "# Pull existing data from GoogleSheet\n",
        "googleSheetID = '1q5sBvxJhJtRwnLcAFVdFbyyVpFZM74vizqwgyQBx2OQ'\n",
        "worksheetName = 'SET_BASE'\n",
        "URL = ('https://docs.google.com/spreadsheets/d/{0}/gviz/tq?tqx=out:csv&sheet={1}'.format(googleSheetID, worksheetName))\n",
        "SET_BASE = pd.read_csv(URL)\n",
        "\n",
        "# Merge data\n",
        "SET = pd.concat([SET_BASE, data_df], axis=0)\n",
        "SET['Date'] = pd.to_datetime(SET['Date']).dt.date\n",
        "SET = SET.drop_duplicates(subset=['Date'], keep='first')\n",
        "SET = SET.sort_values('Date', ascending=True)\n",
        "\n",
        "# Push data to GoogleSheet\n",
        "SET_googleSheet = file.open(\"Thailand Market Breadth\").worksheet(\"SET_BASE\")\n",
        "SET_googleSheet.clear()\n",
        "gd.set_with_dataframe(SET_googleSheet, SET)\n",
        "\n",
        "print('SET Finished')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UbEzyiQuF_P"
      },
      "source": [
        "#### **SET50 Index**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkJ0jpVluH9b",
        "outputId": "93972287-ecee-4cdb-b843-b1b9ba6376c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SET50 Finished\n"
          ]
        }
      ],
      "source": [
        "# SET50\n",
        "driver = web_driver()\n",
        "driver.get('https://www.investing.com/indices/set-50-historical-data')\n",
        "data = driver.page_source\n",
        "data_df = pd.read_html(data)[1]\n",
        "data_df['Date'] = pd.to_datetime(data_df['Date'])\n",
        "data_df = (data_df.rename(columns={'Price':'Close'})[['Date', 'Open', 'High', 'Low', 'Close']]\n",
        "                  .sort_values('Date', ascending=True)\n",
        "                  .reset_index(drop=True))\n",
        "data_df['Date'] = pd.to_datetime(data_df['Date']).dt.date\n",
        "\n",
        "# Pull existing data from GoogleSheet\n",
        "googleSheetID = '1q5sBvxJhJtRwnLcAFVdFbyyVpFZM74vizqwgyQBx2OQ'\n",
        "worksheetName = 'SET50_BASE'\n",
        "URL = ('https://docs.google.com/spreadsheets/d/{0}/gviz/tq?tqx=out:csv&sheet={1}'.format(googleSheetID, worksheetName))\n",
        "SET50_BASE = pd.read_csv(URL)\n",
        "\n",
        "# Merge data\n",
        "SET50 = pd.concat([SET50_BASE, data_df], axis=0)\n",
        "SET50['Date'] = pd.to_datetime(SET50['Date']).dt.date\n",
        "SET50 = SET50.drop_duplicates(subset=['Date'], keep='first')\n",
        "SET50 = SET50.sort_values(by='Date', ascending=True)\n",
        "\n",
        "# Push data to GoogleSheet\n",
        "SET50_googleSheet = file.open(\"Thailand Market Breadth\").worksheet(\"SET50_BASE\")\n",
        "SET50_googleSheet.clear()\n",
        "gd.set_with_dataframe(SET50_googleSheet, SET50)\n",
        "\n",
        "print('SET50 Finished')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xurTs_OHuIUs"
      },
      "source": [
        "#### **SET100 Index**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfDLOqiDuKZl",
        "outputId": "ae7723e9-aec4-41b8-e8e1-4644cd372427"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SET100 Finished\n"
          ]
        }
      ],
      "source": [
        "# SET100\n",
        "driver = web_driver()\n",
        "driver.get('https://www.investing.com/indices/set-100-historical-data')\n",
        "data = driver.page_source\n",
        "data_df = pd.read_html(data)[1]\n",
        "data_df['Date'] = pd.to_datetime(data_df['Date'])\n",
        "data_df = (data_df.rename(columns={'Price':'Close'})[['Date', 'Open', 'High', 'Low', 'Close']]\n",
        "                  .sort_values('Date', ascending=True)\n",
        "                  .reset_index(drop=True))\n",
        "data_df['Date'] = pd.to_datetime(data_df['Date']).dt.date\n",
        "\n",
        "# Pull existing data from GoogleSheet\n",
        "googleSheetID = '1q5sBvxJhJtRwnLcAFVdFbyyVpFZM74vizqwgyQBx2OQ'\n",
        "worksheetName = 'SET100_BASE'\n",
        "URL = ('https://docs.google.com/spreadsheets/d/{0}/gviz/tq?tqx=out:csv&sheet={1}'.format(googleSheetID, worksheetName))\n",
        "SET100_BASE = pd.read_csv(URL)\n",
        "\n",
        "# Merge data\n",
        "SET100 = pd.concat([SET100_BASE, data_df], axis=0)\n",
        "SET100 = SET100.drop_duplicates(subset=['Date'], keep='first')\n",
        "SET100['Date'] = pd.to_datetime(SET100['Date']).dt.date\n",
        "SET100 = SET100.sort_values(by='Date', ascending=True)\n",
        "\n",
        "# Push data to GoogleSheet\n",
        "SET100_googleSheet = file.open(\"Thailand Market Breadth\").worksheet(\"SET100_BASE\")\n",
        "SET100_googleSheet.clear()\n",
        "gd.set_with_dataframe(SET100_googleSheet, SET100)\n",
        "\n",
        "print('SET100 Finished')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byPw2R_zg05T"
      },
      "source": [
        "### **Stock Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rrFbZQ2fs8tl"
      },
      "outputs": [],
      "source": [
        "symbols = ['2S', '3K-BAT', '7UP', 'A', 'AAI', 'AAV', 'ACC', 'ACE', 'ACG', 'ADVANC', 'AEONTS', 'AFC', 'AGE', 'AH', 'AHC', 'AI', 'AIE', 'AIMCG', 'AIMIRT', 'AIT', 'AJ', 'AJA', 'AKR', 'AKS', 'ALLA', 'ALLY', 'ALT', 'ALUCON', 'AMANAH', 'AMARIN', 'AMATA', 'AMATAR', 'AMATAV', 'AMC', 'AMR', 'ANAN', 'AOT', 'AP', 'APCO', 'APCS', 'APEX', 'APURE', 'AQUA', 'AS', 'ASAP', 'ASEFA', 'ASIA', 'ASIAN', 'ASIMAR', 'ASK', 'ASP', 'ASW', 'AURA', 'AWC', 'AYUD', 'B', 'B-WORK', 'B52', 'BA', 'BAFS', 'BAM', 'BANPU', 'BAREIT', 'BAY', 'BBGI', 'BBL', 'BCH', 'BCP', 'BCPG', 'BCT', 'BDMS', 'BEAUTY', 'BEC', 'BEM', 'BEYOND', 'BGC', 'BGRIM', 'BH', 'BIG', 'BIOTEC', 'BIZ', 'BJC', 'BJCHI', 'BKD', 'BKI', 'BKKCP', 'BLA', 'BLAND', 'BLISS', 'BOFFICE', 'BPP', 'BR', 'BRI', 'BROCK', 'BRR', 'BRRGIF', 'BSBM', 'BTG', 'BTNC', 'BTS', 'BTSGIF', 'BUI', 'BWG', 'BYD', 'CBG', 'CCET', 'CCP', 'CEN', 'CENTEL', 'CFRESH', 'CGD', 'CGH', 'CH', 'CHARAN', 'CHASE', 'CHAYO', 'CHG', 'CHOTI', 'CI', 'CIMBT', 'CITY', 'CIVIL', 'CK', 'CKP', 'CM', 'CMAN', 'CMC', 'CMR', 'CNT', 'COM7', 'COTTO', 'CPALL', 'CPF', 'CPH', 'CPI', 'CPL', 'CPN', 'CPNCG', 'CPNREIT', 'CPT', 'CPTGF', 'CPW', 'CRANE', 'CRC', 'CSC', 'CSP', 'CSR', 'CSS', 'CTARAF', 'CTW', 'CV', 'CWT', 'DCC', 'DCON', 'DDD', 'DELTA', 'DEMCO', 'DIF', 'DMT', 'DOHOME', 'DREIT', 'DRT', 'DTCENT', 'DTCI', 'DUSIT', 'EA', 'EASON', 'EASTW', 'ECL', 'EE', 'EGATIF', 'EGCO', 'EKH', 'EMC', 'EP', 'EPG', 'ERW', 'ERWPF', 'ESSO', 'ESTAR', 'ETC', 'EVER', 'FANCY', 'FE', 'FMT', 'FN', 'FNS', 'FORTH', 'FPT', 'FSS', 'FTE', 'FTI', 'FTREIT', 'FUTUREPF', 'GABLE', 'GAHREIT', 'GBX', 'GC', 'GEL', 'GENCO', 'GFPT', 'GGC', 'GIFT', 'GJS', 'GL', 'GLAND', 'GLOBAL', 'GLOCON', 'GPI', 'GPSC', 'GRAMMY', 'GRAND', 'GREEN', 'GROREIT', 'GSTEEL', 'GULF', 'GUNKUL', 'GVREIT', 'GYT', 'HANA', 'HENG', 'HFT', 'HMPRO', 'HPF', 'HTC', 'HTECH', 'HUMAN', 'HYDROGEN', 'ICC', 'ICHI', 'IFEC', 'IFS', 'IHL', 'III', 'ILINK', 'ILM', 'IMPACT', 'INET', 'INETREIT', 'INGRS', 'INOX', 'INSET', 'INSURE', 'INTUCH', 'IRC', 'IRPC', 'IT', 'ITC', 'ITD', 'ITEL', 'IVL', 'J', 'JAS', 'JASIF', 'JCK', 'JCT', 'JDF', 'JKN', 'JMART', 'JMT', 'JR', 'JTS', 'KAMART', 'KBANK', 'KBS', 'KBSPIF', 'KC', 'KCAR', 'KCE', 'KDH', 'KEX', 'KGI', 'KIAT', 'KISS', 'KKC', 'KKP', 'KPNPF', 'KSL', 'KTB', 'KTBSTMR', 'KTC', 'KTIS', 'KWC', 'KWI', 'KYE', 'LALIN', 'LANNA', 'LEE', 'LH', 'LHFG', 'LHHOTEL', 'LHK', 'LHPF', 'LHSC', 'LOXLEY', 'LPF', 'LPH', 'LPN', 'LRH', 'LST', 'LUXF', 'M', 'M-CHAI', 'M-II', 'M-PAT', 'M-STOR', 'MACO', 'MAJOR', 'MAKRO', 'MALEE', 'MANRIN', 'MATCH', 'MATI', 'MAX', 'MBK', 'MC', 'MCOT', 'MCS', 'MDX', 'MEGA', 'MENA', 'METCO', 'MFC', 'MFEC', 'MGC', 'MICRO', 'MIDA', 'MILL', 'MINT', 'MIPF', 'MIT', 'MJD', 'MJLF', 'MK', 'ML', 'MNIT', 'MNIT2', 'MNRF', 'MODERN', 'MONO', 'MOSHI', 'MPIC', 'MSC', 'MST', 'MTC', 'MTI', 'NATION', 'NC', 'NCAP', 'NCH', 'NEP', 'NER', 'NEW', 'NEX', 'NFC', 'NKI', 'NNCL', 'NOBLE', 'NOK', 'NOVA', 'NRF', 'NSL', 'NTV', 'NUSA', 'NV', 'NVD', 'NWR', 'NYT', 'OCC', 'OGC', 'OHTL', 'OISHI', 'ONEE', 'OR', 'ORI', 'OSP', 'PACE', 'PAF', 'PAP', 'PATO', 'PB', 'PCC', 'PCSGH', 'PDJ', 'PEACE', 'PERM', 'PF', 'PG', 'PIN', 'PJW', 'PK', 'PL', 'PLANB', 'PLAT', 'PLE', 'PLUS', 'PM', 'PMTA', 'POLAR', 'POLY', 'POPF', 'PORT', 'POST', 'PPF', 'PPP', 'PPPM', 'PQS', 'PR9', 'PRAKIT', 'PREB', 'PRECHA', 'PRG', 'PRIME', 'PRIN', 'PRINC', 'PRM', 'PRO', 'PROSPECT', 'PRTR', 'PSH', 'PSL', 'PT', 'PTECH', 'PTG', 'PTL', 'PTT', 'PTTEP', 'PTTGC', 'PYLON', 'Q-CON', 'QH', 'QHHR', 'QHOP', 'QHPF', 'QTC', 'RABBIT', 'RAM', 'RATCH', 'RBF', 'RCL', 'RICHY', 'RJH', 'RML', 'ROCK', 'ROH', 'ROJNA', 'RPC', 'RPH', 'RS', 'RSP', 'RT', 'S', 'S&J', 'S11', 'SA', 'SABINA', 'SABUY', 'SAK', 'SAM', 'SAMART', 'SAMCO', 'SAMTEL', 'SAPPE', 'SAT', 'SAUCE', 'SAWAD', 'SAWANG', 'SBNEXT', 'SC', 'SCAP', 'SCB', 'SCC', 'SCCC', 'SCG', 'SCGP', 'SCI', 'SCM', 'SCN', 'SCP', 'SDC', 'SE-ED', 'SEAFCO', 'SENA', 'SFLEX', 'SFP', 'SGC', 'SGP', 'SHANG', 'SHR', 'SHREIT', 'SIAM', 'SINGER', 'SIRI', 'SIRIP', 'SIS', 'SISB', 'SITHAI', 'SJWD', 'SKE', 'SKN', 'SKR', 'SKY', 'SLP', 'SM', 'SMIT', 'SMK', 'SMPC', 'SMT', 'SNC', 'SNNP', 'SNP', 'SO', 'SOLAR', 'SORKON', 'SPACK', 'SPALI', 'SPC', 'SPCG', 'SPG', 'SPI', 'SPRC', 'SPRIME', 'SQ', 'SRICHA', 'SRIPANWA', 'SSC', 'SSF', 'SSP', 'SSPF', 'SSSC', 'SST', 'SSTRT', 'STA', 'STANLY', 'STARK', 'STEC', 'STECH', 'STGT', 'STHAI', 'STI', 'STPI', 'SUC', 'SUN', 'SUPER', 'SUPEREIF', 'SUSCO', 'SUTHA', 'SVI', 'SVOA', 'SVT', 'SYMC', 'SYNEX', 'SYNTEC', 'TAE', 'TASCO', 'TC', 'TCAP', 'TCC', 'TCCC', 'TCJ', 'TCMC', 'TCOAT', 'TEAM', 'TEAMG', 'TEGH', 'TEKA', 'TFFIF', 'TFG', 'TFI', 'TFM', 'TFMAMA', 'TGE', 'TGH', 'TGPRO', 'TH', 'THAI', 'THANI', 'THCOM', 'THE', 'THG', 'THIP', 'THRE', 'THREL', 'TIDLOR', 'TIF1', 'TIPCO', 'TIPH', 'TISCO', 'TK', 'TKC', 'TKN', 'TKS', 'TKT', 'TLHPF', 'TLI', 'TMD', 'TMT', 'TNITY', 'TNL', 'TNPC', 'TNPF', 'TNR', 'TOA', 'TOG', 'TOP', 'TOPP', 'TPA', 'TPAC', 'TPBI', 'TPCS', 'TPIPL', 'TPIPP', 'TPOLY', 'TPP', 'TPRIME', 'TQM', 'TR', 'TRC', 'TRITN', 'TRU', 'TRUBB', 'TRUE', 'TSC', 'TSE', 'TSI', 'TSTE', 'TSTH', 'TTA', 'TTB', 'TTCL', 'TTI', 'TTLPF', 'TTT', 'TTW', 'TU', 'TU-PF', 'TVI', 'TVO', 'TWP', 'TWPC', 'TWZ', 'TYCN', 'UAC', 'UBE', 'UMI', 'UNIQ', 'UOBKH', 'UP', 'UPF', 'UPOIC', 'URBNPF', 'UTP', 'UV', 'UVAN', 'VARO', 'VGI', 'VIBHA', 'VIH', 'VNG', 'VPO', 'VRANDA', 'W', 'WACOAL', 'WAVE', 'WFX', 'WGE', 'WHA', 'WHABT', 'WHAIR', 'WHART', 'WHAUP', 'WICE', 'WIIK', 'WIN', 'WORK', 'WP', 'WPH', 'XPG', 'ZEN']\n",
        "COL_NAME = ['Date', '2S', '3K-BAT', '7UP', 'A', 'AAI', 'AAV', 'ACC', 'ACE', 'ACG', 'ADVANC', 'AEONTS', 'AFC', 'AGE', 'AH', 'AHC', 'AI', 'AIE', 'AIMCG', 'AIMIRT', 'AIT', 'AJ', 'AJA', 'AKR', 'AKS', 'ALLA', 'ALLY', 'ALT', 'ALUCON', 'AMANAH', 'AMARIN', 'AMATA', 'AMATAR', 'AMATAV', 'AMC', 'AMR', 'ANAN', 'AOT', 'AP', 'APCO', 'APCS', 'APEX', 'APURE', 'AQUA', 'AS', 'ASAP', 'ASEFA', 'ASIA', 'ASIAN', 'ASIMAR', 'ASK', 'ASP', 'ASW', 'AURA', 'AWC', 'AYUD', 'B', 'B-WORK', 'B52', 'BA', 'BAFS', 'BAM', 'BANPU', 'BAREIT', 'BAY', 'BBGI', 'BBL', 'BCH', 'BCP', 'BCPG', 'BCT', 'BDMS', 'BEAUTY', 'BEC', 'BEM', 'BEYOND', 'BGC', 'BGRIM', 'BH', 'BIG', 'BIOTEC', 'BIZ', 'BJC', 'BJCHI', 'BKD', 'BKI', 'BKKCP', 'BLA', 'BLAND', 'BLISS', 'BOFFICE', 'BPP', 'BR', 'BRI', 'BROCK', 'BRR', 'BRRGIF', 'BSBM', 'BTG', 'BTNC', 'BTS', 'BTSGIF', 'BUI', 'BWG', 'BYD', 'CBG', 'CCET', 'CCP', 'CEN', 'CENTEL', 'CFRESH', 'CGD', 'CGH', 'CH', 'CHARAN', 'CHASE', 'CHAYO', 'CHG', 'CHOTI', 'CI', 'CIMBT', 'CITY', 'CIVIL', 'CK', 'CKP', 'CM', 'CMAN', 'CMC', 'CMR', 'CNT', 'COM7', 'COTTO', 'CPALL', 'CPF', 'CPH', 'CPI', 'CPL', 'CPN', 'CPNCG', 'CPNREIT', 'CPT', 'CPTGF', 'CPW', 'CRANE', 'CRC', 'CSC', 'CSP', 'CSR', 'CSS', 'CTARAF', 'CTW', 'CV', 'CWT', 'DCC', 'DCON', 'DDD', 'DELTA', 'DEMCO', 'DIF', 'DMT', 'DOHOME', 'DREIT', 'DRT', 'DTCENT', 'DTCI', 'DUSIT', 'EA', 'EASON', 'EASTW', 'ECL', 'EE', 'EGATIF', 'EGCO', 'EKH', 'EMC', 'EP', 'EPG', 'ERW', 'ERWPF', 'ESSO', 'ESTAR', 'ETC', 'EVER', 'FANCY', 'FE', 'FMT', 'FN', 'FNS', 'FORTH', 'FPT', 'FSS', 'FTE', 'FTI', 'FTREIT', 'FUTUREPF', 'GABLE', 'GAHREIT', 'GBX', 'GC', 'GEL', 'GENCO', 'GFPT', 'GGC', 'GIFT', 'GJS', 'GL', 'GLAND', 'GLOBAL', 'GLOCON', 'GPI', 'GPSC', 'GRAMMY', 'GRAND', 'GREEN', 'GROREIT', 'GSTEEL', 'GULF', 'GUNKUL', 'GVREIT', 'GYT', 'HANA', 'HENG', 'HFT', 'HMPRO', 'HPF', 'HTC', 'HTECH', 'HUMAN', 'HYDROGEN', 'ICC', 'ICHI', 'IFEC', 'IFS', 'IHL', 'III', 'ILINK', 'ILM', 'IMPACT', 'INET', 'INETREIT', 'INGRS', 'INOX', 'INSET', 'INSURE', 'INTUCH', 'IRC', 'IRPC', 'IT', 'ITC', 'ITD', 'ITEL', 'IVL', 'J', 'JAS', 'JASIF', 'JCK', 'JCT', 'JDF', 'JKN', 'JMART', 'JMT', 'JR', 'JTS', 'KAMART', 'KBANK', 'KBS', 'KBSPIF', 'KC', 'KCAR', 'KCE', 'KDH', 'KEX', 'KGI', 'KIAT', 'KISS', 'KKC', 'KKP', 'KPNPF', 'KSL', 'KTB', 'KTBSTMR', 'KTC', 'KTIS', 'KWC', 'KWI', 'KYE', 'LALIN', 'LANNA', 'LEE', 'LH', 'LHFG', 'LHHOTEL', 'LHK', 'LHPF', 'LHSC', 'LOXLEY', 'LPF', 'LPH', 'LPN', 'LRH', 'LST', 'LUXF', 'M', 'M-CHAI', 'M-II', 'M-PAT', 'M-STOR', 'MACO', 'MAJOR', 'MAKRO', 'MALEE', 'MANRIN', 'MATCH', 'MATI', 'MAX', 'MBK', 'MC', 'MCOT', 'MCS', 'MDX', 'MEGA', 'MENA', 'METCO', 'MFC', 'MFEC', 'MGC', 'MICRO', 'MIDA', 'MILL', 'MINT', 'MIPF', 'MIT', 'MJD', 'MJLF', 'MK', 'ML', 'MNIT', 'MNIT2', 'MNRF', 'MODERN', 'MONO', 'MOSHI', 'MPIC', 'MSC', 'MST', 'MTC', 'MTI', 'NATION', 'NC', 'NCAP', 'NCH', 'NEP', 'NER', 'NEW', 'NEX', 'NFC', 'NKI', 'NNCL', 'NOBLE', 'NOK', 'NOVA', 'NRF', 'NSL', 'NTV', 'NUSA', 'NV', 'NVD', 'NWR', 'NYT', 'OCC', 'OGC', 'OHTL', 'OISHI', 'ONEE', 'OR', 'ORI', 'OSP', 'PACE', 'PAF', 'PAP', 'PATO', 'PB', 'PCC', 'PCSGH', 'PDJ', 'PEACE', 'PERM', 'PF', 'PG', 'PIN', 'PJW', 'PK', 'PL', 'PLANB', 'PLAT', 'PLE', 'PLUS', 'PM', 'PMTA', 'POLAR', 'POLY', 'POPF', 'PORT', 'POST', 'PPF', 'PPP', 'PPPM', 'PQS', 'PR9', 'PRAKIT', 'PREB', 'PRECHA', 'PRG', 'PRIME', 'PRIN', 'PRINC', 'PRM', 'PRO', 'PROSPECT', 'PRTR', 'PSH', 'PSL', 'PT', 'PTECH', 'PTG', 'PTL', 'PTT', 'PTTEP', 'PTTGC', 'PYLON', 'Q-CON', 'QH', 'QHHR', 'QHOP', 'QHPF', 'QTC', 'RABBIT', 'RAM', 'RATCH', 'RBF', 'RCL', 'RICHY', 'RJH', 'RML', 'ROCK', 'ROH', 'ROJNA', 'RPC', 'RPH', 'RS', 'RSP', 'RT', 'S', 'S11', 'SA', 'SABINA', 'SABUY', 'SAK', 'SAM', 'SAMART', 'SAMCO', 'SAMTEL', 'SAPPE', 'SAT', 'SAUCE', 'SAWAD', 'SAWANG', 'SBNEXT', 'SC', 'SCAP', 'SCB', 'SCC', 'SCCC', 'SCG', 'SCGP', 'SCI', 'SCM', 'SCN', 'SCP', 'SDC', 'SE-ED', 'SEAFCO', 'SENA', 'SFLEX', 'SFP', 'SGC', 'SGP', 'SHANG', 'SHR', 'SHREIT', 'SIAM', 'SINGER', 'SIRI', 'SIRIP', 'SIS', 'SISB', 'SITHAI', 'SJWD', 'SKE', 'SKN', 'SKR', 'SKY', 'SLP', 'SM', 'SMIT', 'SMK', 'SMPC', 'SMT', 'SNC', 'SNNP', 'SNP', 'SO', 'SOLAR', 'SORKON', 'SPACK', 'SPALI', 'SPC', 'SPCG', 'SPG', 'SPI', 'SPRC', 'SPRIME', 'SQ', 'SRICHA', 'SRIPANWA', 'SSC', 'SSF', 'SSP', 'SSPF', 'SSSC', 'SST', 'SSTRT', 'STA', 'STANLY', 'STARK', 'STEC', 'STECH', 'STGT', 'STHAI', 'STI', 'STPI', 'SUC', 'SUN', 'SUPER', 'SUPEREIF', 'SUSCO', 'SUTHA', 'SVI', 'SVOA', 'SVT', 'SYMC', 'SYNEX', 'SYNTEC', 'TAE', 'TASCO', 'TC', 'TCAP', 'TCC', 'TCCC', 'TCJ', 'TCMC', 'TCOAT', 'TEAM', 'TEAMG', 'TEGH', 'TEKA', 'TFFIF', 'TFG', 'TFI', 'TFM', 'TFMAMA', 'TGE', 'TGH', 'TGPRO', 'TH', 'THAI', 'THANI', 'THCOM', 'THE', 'THG', 'THIP', 'THRE', 'THREL', 'TIDLOR', 'TIF1', 'TIPCO', 'TIPH', 'TISCO', 'TK', 'TKC', 'TKN', 'TKS', 'TKT', 'TLHPF', 'TLI', 'TMD', 'TMT', 'TNITY', 'TNL', 'TNPC', 'TNPF', 'TNR', 'TOA', 'TOG', 'TOP', 'TOPP', 'TPA', 'TPAC', 'TPBI', 'TPCS', 'TPIPL', 'TPIPP', 'TPOLY', 'TPP', 'TPRIME', 'TQM', 'TR', 'TRC', 'TRITN', 'TRU', 'TRUBB', 'TRUE', 'TSC', 'TSE', 'TSI', 'TSTE', 'TSTH', 'TTA', 'TTB', 'TTCL', 'TTI', 'TTLPF', 'TTT', 'TTW', 'TU', 'TU-PF', 'TVI', 'TVO', 'TWP', 'TWPC', 'TWZ', 'TYCN', 'UAC', 'UBE', 'UMI', 'UNIQ', 'UOBKH', 'UP', 'UPF', 'UPOIC', 'URBNPF', 'UTP', 'UV', 'UVAN', 'VARO', 'VGI', 'VIBHA', 'VIH', 'VNG', 'VPO', 'VRANDA', 'W', 'WACOAL', 'WAVE', 'WFX', 'WGE', 'WHA', 'WHABT', 'WHAIR', 'WHART', 'WHAUP', 'WICE', 'WIIK', 'WIN', 'WORK', 'WP', 'WPH', 'XPG', 'ZEN']\n",
        "\n",
        "def getGoogleSheet(worksheetName='OPEN'):\n",
        "    googleSheetID = '1q5sBvxJhJtRwnLcAFVdFbyyVpFZM74vizqwgyQBx2OQ'\n",
        "    worksheetName = worksheetName\n",
        "    URL = ('https://docs.google.com/spreadsheets/d/{0}/gviz/tq?tqx=out:csv&sheet={1}'\n",
        "          .format(googleSheetID, worksheetName))\n",
        "    data = pd.read_csv(URL)\n",
        "    data.columns = COL_NAME\n",
        "    return data\n",
        "\n",
        "OPEN = getGoogleSheet(worksheetName='OPEN')\n",
        "HIGH = getGoogleSheet(worksheetName='HIGH')\n",
        "LOW = getGoogleSheet(worksheetName='LOW')\n",
        "CLOSE = getGoogleSheet(worksheetName='CLOSE')\n",
        "VOLUME = getGoogleSheet(worksheetName='VOLUME')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4GJKwWeYtGqe"
      },
      "outputs": [],
      "source": [
        "# Manipulate Data\n",
        "\n",
        "# Open Price\n",
        "SET_O_price = OPEN.copy()\n",
        "SET_O_price = SET_O_price.bfill(axis ='rows')\n",
        "SET_O_price = SET_O_price.ffill(axis ='rows')\n",
        "SET_O_price = SET_O_price.rename(columns={'date': 'Date'})\n",
        "SET_O_price['Date'] = pd.to_datetime(SET_O_price['Date'])\n",
        "SET_O_price['Date'] = SET_O_price['Date'].dt.tz_localize(timezone('Asia/Bangkok')).dt.date\n",
        "SET_O_price = SET_O_price.drop_duplicates(subset='Date', keep='first')\n",
        "\n",
        "# High Price\n",
        "SET_H_price = HIGH.copy()\n",
        "SET_H_price = SET_H_price.bfill(axis ='rows')\n",
        "SET_H_price = SET_H_price.ffill(axis ='rows')\n",
        "SET_H_price = SET_H_price.rename(columns={'date': 'Date'})\n",
        "SET_H_price['Date'] = pd.to_datetime(SET_H_price['Date'])\n",
        "SET_H_price['Date'] = SET_H_price['Date'].dt.tz_localize(timezone('Asia/Bangkok')).dt.date\n",
        "SET_H_price = SET_H_price.drop_duplicates(subset='Date', keep='first')\n",
        "\n",
        "# Low Price\n",
        "SET_L_price = LOW.copy()\n",
        "SET_L_price = SET_L_price.bfill(axis ='rows')\n",
        "SET_L_price = SET_L_price.ffill(axis ='rows')\n",
        "SET_L_price = SET_L_price.rename(columns={'date': 'Date'})\n",
        "SET_L_price['Date'] = pd.to_datetime(SET_L_price['Date'])\n",
        "SET_L_price['Date'] = SET_L_price['Date'].dt.tz_localize(timezone('Asia/Bangkok')).dt.date\n",
        "SET_L_price = SET_L_price.drop_duplicates(subset='Date', keep='first')\n",
        "\n",
        "# Adj. Close Price\n",
        "SET_C_price = CLOSE.copy()\n",
        "SET_C_price = SET_C_price.bfill(axis ='rows')\n",
        "SET_C_price = SET_C_price.ffill(axis ='rows')\n",
        "SET_C_price = SET_C_price.rename(columns={'date': 'Date'})\n",
        "SET_C_price['Date'] = pd.to_datetime(SET_C_price['Date'])\n",
        "SET_C_price['Date'] = SET_C_price['Date'].dt.tz_localize(timezone('Asia/Bangkok')).dt.date\n",
        "SET_C_price = SET_C_price.drop_duplicates(subset='Date', keep='first')\n",
        "\n",
        "# Volume\n",
        "SET_Volume = VOLUME.copy()\n",
        "SET_Volume = SET_Volume.bfill(axis ='rows')\n",
        "SET_Volume = SET_Volume.ffill(axis ='rows')\n",
        "SET_Volume = SET_Volume.rename(columns={'date': 'Date'})\n",
        "SET_Volume['Date'] = pd.to_datetime(SET_Volume['Date'])\n",
        "SET_Volume['Date'] = SET_Volume['Date'].dt.tz_localize(timezone('Asia/Bangkok')).dt.date\n",
        "SET_Volume = SET_Volume.drop_duplicates(subset='Date', keep='first')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxSJyyhEzuFM"
      },
      "source": [
        "### **Market Breadth**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "5ABLcaWuwxMw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "013fe492-eb5a-4dc1-b32d-ad2e00c634a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Time:  2023-06-08 16:22:07.445754\n",
            "Prepare Symbols Finished\n",
            "Function getMarketBreadth Finished\n",
            "SET Breadth Finished\n",
            "SET50 Breadth Finished\n",
            "SET100 Breadth Finished\n",
            "2023-06-08 16:22:42.904367\n",
            "Execution Time:  35.458613\n"
          ]
        }
      ],
      "source": [
        "import datetime as dt\n",
        "start_time = dt.datetime.now()\n",
        "print('Starting Time: ', start_time)\n",
        "\n",
        "SET50_23H1 = ['ADVANC', 'AOT', 'AWC', 'BANPU', 'BBL', 'BDMS', 'BEM', 'BGRIM', 'BH', 'BTS', 'CBG', 'CENTEL', 'COM7', 'CPALL', 'CPF', 'CPN', 'CRC', 'DELTA', 'EA', 'EGCO', 'GLOBAL', 'GPSC', 'GULF', 'HMPRO', 'INTUCH', 'IVL', 'JMART', 'JMT', 'KBANK', 'KTB', 'KTC', 'LH', 'MINT', 'MTC', 'OR', 'OSP', 'PTT', 'PTTEP', 'PTTGC', 'RATCH', 'SCB', 'SCC', 'SCGP', 'SAWAD', 'TIDLOR', 'TISCO', 'TOP', 'TRUE', 'TTB', 'TU']\n",
        "\n",
        "SET50_22H2 = ['ADVANC', 'AOT', 'AWC', 'BANPU', 'BBL', 'BDMS', 'BEM', 'BGRIM', 'BH', 'BLA', 'BTS', 'CBG', 'CPALL', 'CPF', 'CPN', 'CRC', 'DTAC', 'EA', 'EGCO', 'GLOBAL', 'GPSC', 'GULF', 'HMPRO', 'INTUCH', 'IRPC', 'IVL', 'JMART', 'JMT', 'KBANK', 'KCE', 'KTB', 'KTC', 'LH', 'MINT', 'MTC', 'OR', 'OSP', 'PTT', 'PTTEP', 'PTTGC', 'SAWAD', 'SCB', 'SCC', 'SCGP', 'TIDLOR', 'TISCO', 'TOP', 'TRUE', 'TTB', 'TU']\n",
        "SET50_22H1 = ['ADVANC', 'AOT', 'AWC', 'BANPU', 'BBL', 'BDMS', 'BEM', 'BGRIM', 'BH', 'BTS', 'CBG', 'COM7', 'CPALL', 'CPF', 'CPN', 'CRC', 'DTAC', 'EA', 'EGCO', 'GLOBAL', 'GPSC', 'GULF', 'HMPRO', 'INTUCH', 'IRPC', 'IVL', 'KBANK', 'KCE', 'KTB', 'KTC', 'LH', 'MINT', 'MTC', 'OSP', 'PTT', 'PTTEP', 'PTTGC', 'RATCH', 'SAWAD', 'SCB', 'SCC', 'STGT', 'TCAP', 'TIDLOR', 'TISCO', 'TTB', 'TOP', 'TRUE', 'TTW', 'TU']\n",
        "\n",
        "SET50_21H2 = ['ADVANC', 'AOT', 'BBL', 'BDMS', 'BEM', 'BGRIM', 'BH', 'BJC', 'BTS', 'CBG', 'COM7', 'CPALL', 'CPF', 'CPN', 'CRC', 'DELTA', 'DTAC', 'EA', 'EGCO', 'GLOBAL', 'GPSC', 'GULF', 'HMPRO', 'INTUCH', 'IRPC', 'IVL', 'KBANK', 'KCE', 'KTB', 'KTC', 'LH', 'MINT', 'MTC', 'OSP', 'PTT', 'PTTEP', 'PTTGC', 'RATCH', 'SAWAD', 'SCB', 'SCC', 'STA', 'STGT', 'TCAP', 'TISCO', 'TTB', 'TOP', 'TRUE', 'TTW', 'TU']\n",
        "SET50_21H1 = ['ADVANC', 'AOT', 'AWC', 'BAM', 'BBL', 'BDMS', 'BEM', 'BGRIM', 'BH', 'BJC', 'BTS', 'CBG', 'COM7', 'CPALL', 'CPF', 'CPN', 'CRC', 'DELTA', 'DTAC', 'EA', 'EGCO', 'GLOBAL', 'GPSC', 'GULF', 'HMPRO', 'INTUCH', 'IVL', 'KBANK', 'KTB', 'KTC', 'LH', 'MINT', 'MTC', 'OSP', 'PTT', 'PTTEP', 'PTTGC', 'RATCH', 'SAWAD', 'SCB', 'SCC', 'TCAP', 'TISCO', 'TTB', 'TOA', 'TOP', 'TRUE', 'TTW', 'TU', 'VGI']\n",
        "\n",
        "SET50_20H2 = ['ADVANC', 'AOT', 'AWC', 'BBL', 'BDMS', 'BEM', 'BGRIM', 'BH', 'BJC', 'BPP', 'BTS', 'CBG', 'CPALL', 'CPF', 'CPN', 'CRC', 'DTAC', 'EA', 'EGCO', 'GLOBAL', 'GPSC', 'GULF', 'HMPRO', 'INTUCH', 'IRPC', 'IVL', 'KBANK', 'KTB', 'KTC', 'LH', 'MINT', 'MTC', 'OSP', 'PTT', 'PTTEP', 'PTTGC', 'RATCH', 'SAWAD', 'SCB', 'SCC', 'TCAP', 'TISCO', 'TTB', 'TOA', 'TOP', 'TRUE', 'TTW', 'TU', 'VGI', 'WHA']\n",
        "SET50_20H1 = ['ADVANC', 'AOT', 'AWC', 'BANPU', 'BBL', 'BDMS', 'BEM', 'BGRIM', 'BH', 'BJC', 'BPP', 'BTS', 'CBG', 'CPALL', 'CPF', 'CPN', 'DELTA', 'DTAC', 'EA', 'EGCO', 'GLOBAL', 'GPSC', 'GULF', 'HMPRO', 'INTUCH', 'IRPC', 'IVL', 'KBANK', 'KTB', 'KTC', 'LH', 'MINT', 'MTC', 'OSP', 'PTT', 'PTTEP', 'PTTGC', 'RATCH', 'SAWAD', 'SCB', 'SCC', 'TCAP', 'TISCO', 'TTB', 'TOA', 'TOP', 'TRUE', 'TU', 'VGI', 'WHA']\n",
        "\n",
        "SET100_23H1 = ['AAV', 'ACE', 'ADVANC', 'AMATA', 'AOT', 'AP', 'AWC', 'BAM', 'BANPU', 'BBL', 'BCH', 'BCP', 'BCPG', 'BDMS', 'BEC', 'BEM', 'BGRIM', 'BH', 'BLA', 'BTS', 'BYD', 'CBG', 'CENTEL', 'CHG', 'CK', 'CKP', 'COM7', 'CPALL', 'CPF', 'CPN', 'CRC', 'DELTA', 'DOHOME', 'EA', 'EGCO', 'EPG', 'ESSO', 'FORTH', 'GLOBAL', 'GPSC', 'GULF', 'GUNKUL', 'HANA', 'HMPRO', 'INTUCH', 'IRPC', 'IVL', 'JAS', 'JMART', 'JMT', 'KBANK', 'KCE', 'KEX', 'KKP', 'KTB', 'KTC', 'LH', 'MEGA', 'MINT', 'MTC', 'NEX', 'ONEE', 'OR', 'ORI', 'OSP', 'PLANB', 'PSL', 'PTG', 'PTT', 'PTTEP', 'PTTGC', 'QH', 'RATCH', 'RBF', 'RCL', 'SABUY', 'SAWAD', 'SCB', 'SCC', 'SCGP', 'SINGER', 'SIRI', 'SJWD', 'SPALI', 'SPRC', 'STA', 'STGT', 'TCAP', 'THANI', 'THG', 'TIDLOR', 'TIPH', 'TISCO', 'TOP', 'TQM', 'TRUE', 'TTB', 'TU', 'VGI', 'WHA']\n",
        "\n",
        "SET100_22H2 = ['ACE', 'ADVANC', 'AEONTS', 'AMATA', 'AOT', 'AP', 'AWC', 'BAM', 'BANPU', 'BBL', 'BCH', 'BCP', 'BCPG', 'BDMS', 'BEC', 'BEM', 'BGRIM', 'BH', 'BLA', 'BTS', 'CBG', 'CENTEL', 'CHG', 'CK', 'CKP', 'COM7', 'CPALL', 'CPF', 'CPN', 'CRC', 'DOHOME', 'DTAC', 'EA', 'EGCO', 'EPG', 'ESSO', 'FORTH', 'GLOBAL', 'GPSC', 'GULF', 'GUNKUL', 'HANA', 'HMPRO', 'INTUCH', 'IRPC', 'IVL', 'JMART', 'JMT', 'KBANK', 'KCE', 'KEX', 'KKP', 'KTB', 'KTC', 'LH', 'MAJOR', 'MEGA', 'MINT', 'MTC', 'ONEE', 'OR', 'ORI', 'OSP', 'PLANB', 'PSL', 'PTG', 'PTT', 'PTTEP', 'PTTGC', 'QH', 'RATCH', 'RBF', 'RCL', 'SAWAD', 'SCB', 'SCC', 'SCGP', 'SINGER', 'SPALI', 'SPRC', 'STA', 'STARK', 'STEC', 'STGT', 'SUPER', 'SYNEX', 'TASCO', 'TCAP', 'THANI', 'TIDLOR', 'TIPH', 'TISCO', 'TOP', 'TQM', 'TRUE', 'TTA', 'TTB', 'TU', 'VGI', 'WHA']\n",
        "SET100_22H1 = ['ACE', 'ADVANC', 'AEONTS', 'AMATA', 'AOT', 'AP', 'AWC', 'BAM', 'BANPU', 'BBL', 'BCH', 'BCP', 'BCPG', 'BDMS', 'BEC', 'BEM', 'BGRIM', 'BH', 'BLA', 'BPP', 'BTS', 'CBG', 'CENTEL', 'CHG', 'CK', 'CKP', 'COM7', 'CPALL', 'CPF', 'CPN', 'DOHOME', 'DTAC', 'EA', 'EGCO', 'EPG', 'ERW', 'ESSO', 'GLOBAL', 'GPSC', 'GULF', 'GUNKUL', 'HANA', 'HMPRO', 'INTUCH', 'IRPC', 'IVL', 'JMART', 'JMT', 'KBANK', 'KCE', 'KEX', 'KKP', 'KTB', 'KTC', 'LH', 'MAJOR', 'MEGA', 'MINT', 'MTC', 'ORI', 'OSP', 'PLANB', 'PTG', 'PTT', 'PTTEP', 'PTTGC', 'QH', 'RATCH', 'RBF', 'RCL', 'RCL', 'RS', 'SAWAD', 'SCB', 'SCC', 'SINGER', 'SIRI', 'SPALI', 'SPRC', 'STA', 'STARK', 'STEC', 'STGT', 'SUPER', 'SYNEX', 'TASCO', 'TCAP', 'THANI', 'TIDLOR', 'TISCO', 'TKN', 'TMB', 'TOP', 'TQM', 'TRUE', 'TTA', 'TU', 'TVO', 'VGI', 'WHA']\n",
        "\n",
        "SET100_21H2 = ['AAV', 'ACE', 'ADVANC', 'AEONTS', 'AMATA', 'AOT', 'AP', 'BAM', 'BANPU', 'BBL', 'BCH', 'BCP', 'BCPG', 'BDMS', 'BEC', 'BEM', 'BGRIM', 'BH', 'BJC', 'BTS', 'CBG', 'CENTEL', 'CHG', 'CK', 'CKP', 'COM7', 'CPALL', 'CPF', 'CPN', 'DELTA', 'DOHOME', 'DTAC', 'EA', 'EGCO', 'ERW', 'ESSO', 'GLOBAL', 'GPSC', 'GULF', 'GUNKUL', 'HANA', 'HMPRO', 'ICHI', 'INTUCH', 'IRPC', 'IVL', 'JAS', 'JMART', 'JMT', 'KBANK', 'KCE', 'KKP', 'KTB', 'KTC', 'LH', 'MAJOR', 'MEGA', 'MINT', 'MTC', 'NRF', 'ORI', 'OSP', 'PLANB', 'PRM', 'PSL', 'PTG', 'PTL', 'PTT', 'PTTEP', 'PTTGC', 'QH', 'RATCH', 'RBF', 'RCL', 'RS', 'SAWAD', 'SCB', 'SCC', 'SINGER', 'SPALI', 'SPRC', 'STA', 'STEC', 'STGT', 'SUPER', 'SYNEX', 'TASCO', 'TCAP', 'THANI', 'TISCO', 'TKN', 'TKN', 'TMB', 'TOP', 'TQM', 'TRUE', 'TU', 'TVO', 'VGI', 'WHA']\n",
        "SET100_21H1 = ['ACE', 'ADVANC', 'AEONTS', 'AMATA', 'AOT', 'AP', 'AWC', 'BAM', 'BANPU', 'BBL', 'BCH', 'BCP', 'BCPG', 'BDMS', 'BEC', 'BEM', 'BGRIM', 'BH', 'BJC', 'BPP', 'BTS', 'CBG', 'CENTEL', 'CHG', 'CK', 'CKP', 'COM7', 'CPALL', 'CPF', 'CPN', 'DELTA', 'DOHOME', 'DTAC', 'EA', 'EGCO', 'EPG', 'ERW', 'ESSO', 'GFPT', 'GLOBAL', 'GPSC', 'GULF', 'GUNKUL', 'HANA', 'HMPRO', 'INTUCH', 'IRPC', 'IVL', 'JAS', 'JMART', 'JMT', 'KBANK', 'KCE', 'KKP', 'KTB', 'KTC', 'LH', 'MAJOR', 'MBK', 'MEGA', 'MINT', 'MTC', 'ORI', 'OSP', 'PLANB', 'PRM', 'PTG', 'PTT', 'PTTEP', 'PTTGC', 'QH', 'RATCH', 'RBF', 'RCL', 'RS', 'SAWAD', 'SCB', 'SCC', 'SPALI', 'SPRC', 'STA', 'STEC', 'SUPER', 'TASCO', 'TCAP', 'THANI', 'TISCO', 'TKN', 'TMB', 'TOA', 'TOP', 'TPIPP', 'TQM', 'TRUE', 'TTW', 'TU', 'TVO', 'VGI', 'WHA', 'WHAUP']\n",
        "\n",
        "SET100_20H2 = ['AAV', 'ACE', 'ADVANC', 'AEONTS', 'AMATA', 'AOT', 'AP', 'AWC', 'BANPU', 'BBL', 'BCH', 'BCP', 'BCPG', 'BDMS', 'BEC', 'BEM', 'BGRIM', 'BH', 'BJC', 'BPP', 'BTS', 'CBG', 'CENTEL', 'CHG', 'CK', 'CKP', 'COM7', 'CPALL', 'CPF', 'CPN', 'DOHOME', 'DTAC', 'EA', 'EGCO', 'EPG', 'ERW', 'ESSO', 'GFPT', 'GLOBAL', 'GPSC', 'GULF', 'GUNKUL', 'HANA', 'HMPRO', 'INTUCH', 'IRPC', 'IVL', 'JAS', 'JMT', 'KBANK', 'KCE', 'KKP', 'KTB', 'KTC', 'LH', 'MAJOR', 'MEGA', 'MINT', 'MTC', 'ORI', 'OSP', 'PLANB', 'PRM', 'PSH', 'PTG', 'PTT', 'PTTEP', 'PTTGC', 'QH', 'RATCH', 'RBF', 'RCL', 'RS', 'SAWAD', 'SCB', 'SCC', 'SCG', 'SIRI', 'SPALI', 'SPRC', 'STA', 'STEC', 'SUPER', 'TASCO', 'TCAP', 'THANI', 'TISCO', 'TKN', 'TMB', 'TOA', 'TOP', 'TPIPP', 'TQM', 'TRUE', 'TTW', 'TU', 'TVO', 'VGI', 'WHA', 'WHAUP']\n",
        "SET100_20H1 = ['AAV', 'ADVANC', 'AEONTS', 'AMATA', 'AOT', 'AP', 'AWC', 'BANPU', 'BBL', 'BCH', 'BCP', 'BCPG', 'BDMS', 'BEC', 'BEM', 'BGRIM', 'BH', 'BJC', 'BPP', 'BTS', 'CBG', 'CENTEL', 'CHG', 'CK', 'CKP', 'COM7', 'CPALL', 'CPF', 'CPN', 'DELTA', 'DTAC', 'EA', 'EGCO', 'EPG', 'ERW', 'ESSO', 'GFPT', 'GLOBAL', 'GPSC', 'GULF', 'GUNKUL', 'HANA', 'HMPRO', 'INTUCH', 'IRPC', 'IVL', 'JAS', 'JMT', 'KBANK', 'KCE', 'KKP', 'KTB', 'KTC', 'LH', 'MAJOR', 'MBK', 'MEGA', 'MINT', 'MTC', 'ORI', 'OSP', 'PLANB', 'PRM', 'PSH', 'PSL', 'PTG', 'PTT', 'PTTEP', 'PTTGC', 'QH', 'RATCH', 'RCL', 'RS', 'SAWAD', 'SCB', 'SCC', 'SCG', 'SPALI', 'SPRC', 'STA', 'STEC', 'STPI', 'SUPER', 'TASCO', 'TCAP', 'THAI', 'THANI', 'THG', 'TISCO', 'TKN', 'TMB', 'TOA', 'TOP', 'TPIPP', 'TQM', 'TRUE', 'TTW', 'TU', 'VGI', 'WHA']\n",
        "\n",
        "print('Prepare Symbols Finished')\n",
        "\n",
        "# Advanced Unchanged Declined\n",
        "def getAdvUncDec(Close):\n",
        "  AdvUncDec = pd.DataFrame()\n",
        "  AdvUncDec['Date'] = Close.Date\n",
        "  AdvUncDec['Advanced'] = pd.DataFrame((Close.drop(columns=['Date']).pct_change().drop([0]) > 0).sum(axis=1), columns=['Advanced'])\n",
        "  AdvUncDec['Unchanged'] = pd.DataFrame((Close.drop(columns=['Date']).pct_change().drop([0]) == 0).sum(axis=1), columns=['Unchanged'])\n",
        "  AdvUncDec['Declined'] = pd.DataFrame((Close.drop(columns=['Date']).pct_change().drop([0]) < 0).sum(axis=1), columns=['Declined'])\n",
        "  AdvUncDec['Total'] = AdvUncDec['Advanced'] + AdvUncDec['Unchanged'] + AdvUncDec['Declined']\n",
        "  AdvUncDec['Date'] = pd.to_datetime(AdvUncDec['Date']).dt.date\n",
        "  AdvUncDec = AdvUncDec.dropna().reset_index(drop=True)\n",
        "  AdvUncDec['pct_Adv'] = AdvUncDec.Advanced/AdvUncDec.Total\n",
        "  AdvUncDec['pct_Unc'] = AdvUncDec.Unchanged/AdvUncDec.Total\n",
        "  AdvUncDec['pct_Dec'] = AdvUncDec.Declined/AdvUncDec.Total\n",
        "\n",
        "  return AdvUncDec\n",
        "\n",
        "# New High - New Low\n",
        "def getNewHighNewLow(High, Low):\n",
        "  period_length = [20, 60, 200, 250]\n",
        "  cut_rows = 250 #max(period_length)\n",
        "\n",
        "  NewHigh = pd.DataFrame()\n",
        "  NewHigh['Date'] = High['Date'].iloc[1:]\n",
        "  High_without_date = High.drop(columns=['Date'])\n",
        "  High_shifted = High_without_date.shift()\n",
        "\n",
        "  for length in period_length:\n",
        "      NewHigh['NH' + str(length)] = (High_without_date > High_shifted.rolling(length).max()).sum(axis=1).iloc[1:]\n",
        "\n",
        "  NewLow = pd.DataFrame()\n",
        "  NewLow['Date'] = Low['Date'].iloc[1:]\n",
        "  Low_without_date = Low.drop(columns=['Date'])\n",
        "  Low_shifted = Low_without_date.shift()\n",
        "\n",
        "  for length in period_length:\n",
        "      NewLow['NL' + str(length)] = (Low_without_date < Low_shifted.rolling(length).min()).sum(axis=1).iloc[1:]\n",
        "\n",
        "  NHNL = NewHigh.merge(NewLow, on='Date', how='inner').tail(High.shape[0] - cut_rows).reset_index(drop=True)\n",
        "\n",
        "  return NHNL\n",
        "\n",
        "# Moving Average\n",
        "def getMovingAvg(Close):\n",
        "  ma_length = [20, 60, 200]\n",
        "  cut_rows = 200 #max(ma_length)\n",
        "\n",
        "  moving_avg_columns = ['MA' + str(length) for length in ma_length]\n",
        "  moving_avg_values = pd.DataFrame()\n",
        "\n",
        "  moving_avg_values['Date'] = Close['Date']\n",
        "\n",
        "  for length in ma_length:\n",
        "      moving_avg_values['MA' + str(length)] = (Close.drop(columns=['Date']) > Close.drop(columns=['Date']).rolling(length).mean()).sum(axis=1)\n",
        "\n",
        "  MovingAvg = moving_avg_values[moving_avg_columns].iloc[cut_rows:,:]\n",
        "  MovingAvg['Date'] = moving_avg_values['Date']\n",
        "  MovingAvg = MovingAvg[['Date', 'MA'+str(ma_length[0]), 'MA'+str(ma_length[1]), 'MA'+str(ma_length[2])]].reset_index(drop=True)\n",
        "\n",
        "  return MovingAvg\n",
        "\n",
        "# MACD - Signal\n",
        "def ewma_py(x, n):\n",
        "  alpha = 2/(1+n)\n",
        "  y = np.zeros_like(x)\n",
        "  y[0] = x[0]\n",
        "  for i in range(1, len(x)):\n",
        "      y[i] = alpha * x[i] + (1-alpha) * y[i-1]\n",
        "  return y\n",
        "\n",
        "def getMACD(Close):\n",
        "  EMA12 = Close.drop(columns=['Date'])\n",
        "  EMA26 = Close.drop(columns=['Date'])\n",
        "\n",
        "  def ewma_py(x, n):\n",
        "    alpha = 2/(1+n)\n",
        "    y = np.zeros_like(x)\n",
        "    y[0] = x[0]\n",
        "    for i in range(1, len(x)):\n",
        "        y[i] = alpha * x[i] + (1-alpha) * y[i-1]\n",
        "    return y\n",
        "\n",
        "  EMA12 = EMA12.apply(lambda column: pd.Series(ewma_py(column.values, 12), index=column.index))\n",
        "  EMA26 = EMA26.apply(lambda column: pd.Series(ewma_py(column.values, 26), index=column.index))\n",
        "  MACD = EMA12 - EMA26\n",
        "\n",
        "  Sinnal_copy = MACD.copy()\n",
        "  Signal = Sinnal_copy.apply(lambda column: pd.Series(ewma_py(column.values, 9), index=column.index))\n",
        "\n",
        "  Signal = pd.DataFrame((MACD > Signal).sum(axis=1), columns=['Signal'])\n",
        "  MACD = pd.DataFrame((MACD > 0).sum(axis=1), columns=['MACD'])\n",
        "  MACD['Date'] = Close['Date']\n",
        "  MACD['Signal'] = Signal['Signal']\n",
        "  columns = MACD.columns.tolist()\n",
        "  columns = [columns[-1]] + columns[:-1]\n",
        "  MACD = MACD[columns]\n",
        "\n",
        "  return MACD\n",
        "\n",
        "def getMarketBreadth(index, High, Low, Close):\n",
        "  AdvUncDec = getAdvUncDec(Close)\n",
        "  NHNL = getNewHighNewLow(High, Low)\n",
        "  MovingAvg = getMovingAvg(Close)\n",
        "  MACD = getMACD(Close)\n",
        "\n",
        "  index['Date'] = pd.to_datetime(index['Date']).dt.date\n",
        "  index = index.reset_index(drop=True)\n",
        "\n",
        "  mergeBreadth_1 = AdvUncDec.merge(NHNL, on='Date', how='inner')\n",
        "  mergeBreadth_2 = MovingAvg.merge(MACD, on='Date', how='inner')\n",
        "  mergeBreadth_3 = mergeBreadth_1.merge(mergeBreadth_2, on='Date', how='inner')\n",
        "  MarketBreadth = index.merge(mergeBreadth_3, on='Date', how='inner')\n",
        "\n",
        "  # Advance - Decline Line\n",
        "  MarketBreadth['Diff'] = (MarketBreadth.Advanced - MarketBreadth.Declined)/(MarketBreadth.Unchanged + 1)\n",
        "  MarketBreadth['DiffSqrt'] = np.sqrt(np.abs(MarketBreadth['Diff'])) * np.sign(MarketBreadth['Diff'])\n",
        "  MarketBreadth['AD_Line'] = MarketBreadth['DiffSqrt'].cumsum()\n",
        "  MarketBreadth = MarketBreadth.drop(['Diff', 'DiffSqrt'], axis=1)\n",
        "\n",
        "  # Breadth Thrust\n",
        "  MarketBreadth['Breadth_Thrust'] = ewma_py((MarketBreadth.Advanced/(MarketBreadth.Advanced + MarketBreadth.Declined)), 10)\n",
        "\n",
        "  # McClellan Oscillator (p.74)\n",
        "  MarketBreadth['EMA19'] = ewma_py((MarketBreadth.Advanced - MarketBreadth.Declined), 19)\n",
        "  MarketBreadth['EMA39'] = ewma_py((MarketBreadth.Advanced - MarketBreadth.Declined), 39)\n",
        "  MarketBreadth['McClellanOscillator'] = MarketBreadth.EMA19 - MarketBreadth.EMA39\n",
        "  MarketBreadth = MarketBreadth.drop(['EMA19', 'EMA39'], axis=1)\n",
        "\n",
        "  # McClellan Summation Index (p.78)\n",
        "  MarketBreadth['McClellanSummationIndex'] = MarketBreadth.McClellanOscillator.cumsum()\n",
        "\n",
        "  # Percentage New High - New Low\n",
        "  period_length = [20, 60, 200, 250]\n",
        "  MarketBreadth['pct_NH'+str(period_length[0])] = MarketBreadth['NH'+str(period_length[0])]/MarketBreadth.Total\n",
        "  MarketBreadth['pct_NH'+str(period_length[1])] = MarketBreadth['NH'+str(period_length[1])]/MarketBreadth.Total\n",
        "  MarketBreadth['pct_NH'+str(period_length[2])] = MarketBreadth['NH'+str(period_length[2])]/MarketBreadth.Total\n",
        "  MarketBreadth['pct_NH'+str(period_length[3])] = MarketBreadth['NH'+str(period_length[3])]/MarketBreadth.Total\n",
        "\n",
        "  MarketBreadth['pct_NL'+str(period_length[0])] = MarketBreadth['NL'+str(period_length[0])]/MarketBreadth.Total\n",
        "  MarketBreadth['pct_NL'+str(period_length[1])] = MarketBreadth['NL'+str(period_length[1])]/MarketBreadth.Total\n",
        "  MarketBreadth['pct_NL'+str(period_length[2])] = MarketBreadth['NL'+str(period_length[2])]/MarketBreadth.Total\n",
        "  MarketBreadth['pct_NL'+str(period_length[3])] = MarketBreadth['NL'+str(period_length[3])]/MarketBreadth.Total\n",
        "\n",
        "  # Diff. Ratio NHNL\n",
        "  MarketBreadth['RatioNHNL200d'] = (MarketBreadth['NH200']/(MarketBreadth['NH200'] + MarketBreadth['NL200'])).rolling(60).mean()\n",
        "  MarketBreadth['Threshold_line'] = MarketBreadth['RatioNHNL200d'].rolling(60).mean()\n",
        "\n",
        "  # Diff ShortLong NHNL\n",
        "  MarketBreadth['diffShortNHNL'] = MarketBreadth['NH60'] - MarketBreadth['NL60']\n",
        "  MarketBreadth['diffLongNHNL'] = MarketBreadth['NH200'] - MarketBreadth['NL200']\n",
        "  MarketBreadth['diffShortLongNHNL'] = MarketBreadth['diffShortNHNL'] - MarketBreadth['diffLongNHNL']\n",
        "  MarketBreadth['diffShortLongNHNL_MA120'] = MarketBreadth['diffShortLongNHNL'].rolling(120).mean()\n",
        "\n",
        "  # Diff. Short-Long New High - New Low (New Highs and New Lows Oscillator, p.134)\n",
        "  HL = (MarketBreadth['pct_NH'+str(period_length[1])] - MarketBreadth['pct_NL'+str(period_length[1])])\n",
        "  MarketBreadth['NHNL_EMA19'] = ewma_py(HL, 19)\n",
        "  MarketBreadth['NHNL_EMA39'] = ewma_py(HL, 39)\n",
        "  MarketBreadth['NewHighsNewLowsOscillator'] = MarketBreadth.NHNL_EMA19 - MarketBreadth.NHNL_EMA39\n",
        "  MarketBreadth = MarketBreadth.drop(['NHNL_EMA19', 'NHNL_EMA39'], axis=1)\n",
        "\n",
        "  MarketBreadth['Date'] = pd.to_datetime(MarketBreadth['Date'])\n",
        "\n",
        "  return MarketBreadth\n",
        "\n",
        "print('Function getMarketBreadth Finished')\n",
        "\n",
        "filter_date = (dt.datetime.now() - dt.timedelta(days=100)).strftime('%Y-%m-%d')\n",
        "SET_Market_Breadth = getMarketBreadth(SET, SET_H_price, SET_L_price, SET_C_price).dropna().reset_index(drop=True)\n",
        "\n",
        "SET_Market_Breadth = SET_Market_Breadth.query(\"(Date >= @filter_date)\").reset_index(drop=True)\n",
        "SET_Market_Breadth = SET_Market_Breadth.drop_duplicates(subset='Date', keep='first')\n",
        "SET_Market_Breadth = SET_Market_Breadth.sort_values(by='Date', ascending=True)\n",
        "\n",
        "SET_Market_Breadth['Date'] = SET_Market_Breadth['Date'].dt.date\n",
        "\n",
        "SET_BREADTH_googleSheet = file.open(\"Thailand Market Breadth\").worksheet(\"SET_BREADTH\")\n",
        "SET_BREADTH_googleSheet.clear()\n",
        "gd.set_with_dataframe(SET_BREADTH_googleSheet, SET_Market_Breadth)\n",
        "\n",
        "print('SET Breadth Finished')\n",
        "\n",
        "# SET50_23H1\n",
        "SET50_23H1_H = pd.concat([SET_H_price.Date, SET_H_price.filter(SET50_23H1)], axis=1)\n",
        "SET50_23H1_L = pd.concat([SET_L_price.Date, SET_L_price.filter(SET50_23H1)], axis=1)\n",
        "SET50_23H1_C = pd.concat([SET_C_price.Date, SET_C_price.filter(SET50_23H1)], axis=1)\n",
        "\n",
        "SET50_23H1_MKB = getMarketBreadth(SET50, SET50_23H1_H, SET50_23H1_L, SET50_23H1_C).reset_index(drop=True)\n",
        "SET50_23H1_MKB = SET50_23H1_MKB.query(\"Date >= '2023-01-01'\").reset_index(drop=True)\n",
        "\n",
        "# SET50_22H2\n",
        "SET50_22H2_H = pd.concat([SET_H_price.Date, SET_H_price.filter(SET50_22H2)], axis=1)\n",
        "SET50_22H2_L = pd.concat([SET_L_price.Date, SET_L_price.filter(SET50_22H2)], axis=1)\n",
        "SET50_22H2_C = pd.concat([SET_C_price.Date, SET_C_price.filter(SET50_22H2)], axis=1)\n",
        "\n",
        "SET50_22H2_MKB = getMarketBreadth(SET50, SET50_22H2_H, SET50_22H2_L, SET50_22H2_C).reset_index(drop=True)\n",
        "SET50_22H2_MKB = SET50_22H2_MKB.query(\"(Date >= '2022-07-01')&(Date <= '2022-12-31')\").reset_index(drop=True)\n",
        "\n",
        "# SET50_22H1\n",
        "SET50_22H1_H = pd.concat([SET_H_price.Date, SET_H_price.filter(SET50_22H1)], axis=1)\n",
        "SET50_22H1_L = pd.concat([SET_L_price.Date, SET_L_price.filter(SET50_22H1)], axis=1)\n",
        "SET50_22H1_C = pd.concat([SET_C_price.Date, SET_C_price.filter(SET50_22H1)], axis=1)\n",
        "\n",
        "SET50_22H1_MKB = getMarketBreadth(SET50, SET50_22H1_H, SET50_22H1_L, SET50_22H1_C).reset_index(drop=True)\n",
        "SET50_22H1_MKB = SET50_22H1_MKB.query(\"(Date >= '2022-01-01')&(Date <= '2022-06-30')\").reset_index(drop=True)\n",
        "\n",
        "# SET50_21H2\n",
        "SET50_21H2_H = pd.concat([SET_H_price.Date, SET_H_price.filter(SET50_21H2)], axis=1)\n",
        "SET50_21H2_L = pd.concat([SET_L_price.Date, SET_L_price.filter(SET50_21H2)], axis=1)\n",
        "SET50_21H2_C = pd.concat([SET_C_price.Date, SET_C_price.filter(SET50_21H2)], axis=1)\n",
        "\n",
        "SET50_21H2_MKB = getMarketBreadth(SET50, SET50_21H2_H, SET50_21H2_L, SET50_21H2_C).reset_index(drop=True)\n",
        "SET50_21H2_MKB = SET50_21H2_MKB.query(\"(Date >= '2021-07-01')&(Date <= '2021-12-31')\").reset_index(drop=True)\n",
        "\n",
        "# SET50_21H1\n",
        "SET50_21H1_H = pd.concat([SET_H_price.Date, SET_H_price.filter(SET50_21H1)], axis=1)\n",
        "SET50_21H1_L = pd.concat([SET_L_price.Date, SET_L_price.filter(SET50_21H1)], axis=1)\n",
        "SET50_21H1_C = pd.concat([SET_C_price.Date, SET_C_price.filter(SET50_21H1)], axis=1)\n",
        "\n",
        "SET50_21H1_MKB = getMarketBreadth(SET50, SET50_21H1_H, SET50_21H1_L, SET50_21H1_C).reset_index(drop=True)\n",
        "SET50_21H1_MKB = SET50_21H1_MKB.query(\"(Date >= '2021-01-01')&(Date <= '2021-06-30')\").reset_index(drop=True)\n",
        "\n",
        "# SET50_20H2\n",
        "SET50_20H2_H = pd.concat([SET_H_price.Date, SET_H_price.filter(SET50_20H2)], axis=1)\n",
        "SET50_20H2_L = pd.concat([SET_L_price.Date, SET_L_price.filter(SET50_20H2)], axis=1)\n",
        "SET50_20H2_C = pd.concat([SET_C_price.Date, SET_C_price.filter(SET50_20H2)], axis=1)\n",
        "\n",
        "SET50_20H2_MKB = getMarketBreadth(SET50, SET50_20H2_H, SET50_20H2_L, SET50_20H2_C).reset_index(drop=True)\n",
        "SET50_20H2_MKB = SET50_20H2_MKB.query(\"(Date >= '2020-07-01')&(Date <= '2020-12-31')\").reset_index(drop=True)\n",
        "\n",
        "# SET50_20H1\n",
        "SET50_20H1_H = pd.concat([SET_H_price.Date, SET_H_price.filter(SET50_20H1)], axis=1)\n",
        "SET50_20H1_L = pd.concat([SET_L_price.Date, SET_L_price.filter(SET50_20H1)], axis=1)\n",
        "SET50_20H1_C = pd.concat([SET_C_price.Date, SET_C_price.filter(SET50_20H1)], axis=1)\n",
        "\n",
        "SET50_20H1_MKB = getMarketBreadth(SET50, SET50_20H1_H, SET50_20H1_L, SET50_20H1_C).reset_index(drop=True)\n",
        "SET50_20H1_MKB = SET50_20H1_MKB.query(\"(Date >= '2020-01-01')&(Date <= '2020-06-30')\").reset_index(drop=True)\n",
        "\n",
        "SET50_BREADTH = (pd.concat([SET50_20H1_MKB, SET50_20H2_MKB,\n",
        "                            SET50_21H1_MKB, SET50_21H2_MKB,\n",
        "                            SET50_22H1_MKB, SET50_22H2_MKB,\n",
        "                            SET50_23H1_MKB], axis=0)\n",
        "                      .drop_duplicates(subset=['Date'], keep='first')\n",
        "                      .sort_values(by='Date',ascending=True))\n",
        "SET50_BREADTH = SET50_BREADTH.query(\"(Date >= @filter_date)\").reset_index(drop=True)\n",
        "SET50_BREADTH = SET50_BREADTH.drop_duplicates(subset='Date', keep='first')\n",
        "SET50_BREADTH = SET50_BREADTH.sort_values(by='Date', ascending=True)\n",
        "SET50_BREADTH['Date'] = SET50_BREADTH['Date'].dt.date\n",
        "\n",
        "\n",
        "SET50_BREADTH_googleSheet = file.open(\"Thailand Market Breadth\").worksheet(\"SET50_BREADTH\")\n",
        "SET50_BREADTH_googleSheet.clear()\n",
        "gd.set_with_dataframe(SET50_BREADTH_googleSheet, SET50_BREADTH)\n",
        "\n",
        "print('SET50 Breadth Finished')\n",
        "\n",
        "# SET100_23H1\n",
        "SET100_23H1_H = pd.concat([SET_H_price.Date, SET_H_price.filter(SET100_23H1)], axis=1)\n",
        "SET100_23H1_L = pd.concat([SET_L_price.Date, SET_L_price.filter(SET100_23H1)], axis=1)\n",
        "SET100_23H1_C = pd.concat([SET_C_price.Date, SET_C_price.filter(SET100_23H1)], axis=1)\n",
        "\n",
        "SET100_23H1_MKB = getMarketBreadth(SET100, SET100_23H1_H, SET100_23H1_L, SET100_23H1_C).reset_index(drop=True)\n",
        "SET100_23H1_MKB = SET100_23H1_MKB.query(\"Date >= '2023-01-01'\").reset_index(drop=True)\n",
        "\n",
        "# SET100_22H2\n",
        "SET100_22H2_H = pd.concat([SET_H_price.Date, SET_H_price.filter(SET100_22H2)], axis=1)\n",
        "SET100_22H2_L = pd.concat([SET_L_price.Date, SET_L_price.filter(SET100_22H2)], axis=1)\n",
        "SET100_22H2_C = pd.concat([SET_C_price.Date, SET_C_price.filter(SET100_22H2)], axis=1)\n",
        "\n",
        "SET100_22H2_MKB = getMarketBreadth(SET100, SET100_22H2_H, SET100_22H2_L, SET100_22H2_C).reset_index(drop=True)\n",
        "SET100_22H2_MKB = SET100_22H2_MKB.query(\"(Date >= '2022-07-01')&(Date <= '2022-12-31')\").reset_index(drop=True)\n",
        "\n",
        "# SET100_22H1\n",
        "SET100_22H1_H = pd.concat([SET_H_price.Date, SET_H_price.filter(SET100_22H1)], axis=1)\n",
        "SET100_22H1_L = pd.concat([SET_L_price.Date, SET_L_price.filter(SET100_22H1)], axis=1)\n",
        "SET100_22H1_C = pd.concat([SET_C_price.Date, SET_C_price.filter(SET100_22H1)], axis=1)\n",
        "\n",
        "SET100_22H1_MKB = getMarketBreadth(SET100, SET100_22H1_H, SET100_22H1_L, SET100_22H1_C).reset_index(drop=True)\n",
        "SET100_22H1_MKB = SET100_22H1_MKB.query(\"(Date >= '2022-01-01')&(Date <= '2022-06-30')\").reset_index(drop=True)\n",
        "\n",
        "# SET100_21H2\n",
        "SET100_21H2_H = pd.concat([SET_H_price.Date, SET_H_price.filter(SET100_21H2)], axis=1)\n",
        "SET100_21H2_L = pd.concat([SET_L_price.Date, SET_L_price.filter(SET100_21H2)], axis=1)\n",
        "SET100_21H2_C = pd.concat([SET_C_price.Date, SET_C_price.filter(SET100_21H2)], axis=1)\n",
        "\n",
        "SET100_21H2_MKB = getMarketBreadth(SET100, SET100_21H2_H, SET100_21H2_L, SET100_21H2_C).reset_index(drop=True)\n",
        "SET100_21H2_MKB = SET100_21H2_MKB.query(\"(Date >= '2021-07-01')&(Date <= '2021-12-31')\").reset_index(drop=True)\n",
        "\n",
        "# SET100_21H1\n",
        "SET100_21H1_H = pd.concat([SET_H_price.Date, SET_H_price.filter(SET100_21H1)], axis=1)\n",
        "SET100_21H1_L = pd.concat([SET_L_price.Date, SET_L_price.filter(SET100_21H1)], axis=1)\n",
        "SET100_21H1_C = pd.concat([SET_C_price.Date, SET_C_price.filter(SET100_21H1)], axis=1)\n",
        "\n",
        "SET100_21H1_MKB = getMarketBreadth(SET100, SET100_21H1_H, SET100_21H1_L, SET100_21H1_C).reset_index(drop=True)\n",
        "SET100_21H1_MKB = SET100_21H1_MKB.query(\"(Date >= '2021-01-01')&(Date <= '2021-06-30')\").reset_index(drop=True)\n",
        "\n",
        "# SET100_20H2\n",
        "SET100_20H2_H = pd.concat([SET_H_price.Date, SET_H_price.filter(SET100_20H2)], axis=1)\n",
        "SET100_20H2_L = pd.concat([SET_L_price.Date, SET_L_price.filter(SET100_20H2)], axis=1)\n",
        "SET100_20H2_C = pd.concat([SET_C_price.Date, SET_C_price.filter(SET100_20H2)], axis=1)\n",
        "\n",
        "SET100_20H2_MKB = getMarketBreadth(SET100, SET100_20H2_H, SET100_20H2_L, SET100_20H2_C).reset_index(drop=True)\n",
        "SET100_20H2_MKB = SET100_20H2_MKB.query(\"(Date >= '2020-07-01')&(Date <= '2020-12-31')\").reset_index(drop=True)\n",
        "\n",
        "# SET100_20H1\n",
        "SET100_20H1_H = pd.concat([SET_H_price.Date, SET_H_price.filter(SET100_20H1)], axis=1)\n",
        "SET100_20H1_L = pd.concat([SET_L_price.Date, SET_L_price.filter(SET100_20H1)], axis=1)\n",
        "SET100_20H1_C = pd.concat([SET_C_price.Date, SET_C_price.filter(SET100_20H1)], axis=1)\n",
        "\n",
        "SET100_20H1_MKB = getMarketBreadth(SET100, SET100_20H1_H, SET100_20H1_L, SET100_20H1_C).reset_index(drop=True)\n",
        "SET100_20H1_MKB = SET100_20H1_MKB.query(\"(Date >= '2020-01-01')&(Date <= '2020-06-30')\").reset_index(drop=True)\n",
        "\n",
        "SET100_BREADTH = (pd.concat([SET100_20H1_MKB, SET100_20H2_MKB,\n",
        "                             SET100_21H1_MKB, SET100_21H2_MKB,\n",
        "                             SET100_22H1_MKB, SET100_22H2_MKB,\n",
        "                             SET100_23H1_MKB], axis=0)\n",
        "                  .drop_duplicates(subset=['Date'], keep='first')\n",
        "                  .sort_values(by='Date',ascending=True))\n",
        "\n",
        "SET100_BREADTH = SET100_BREADTH.query(\"(Date >= @filter_date)\").reset_index(drop=True)\n",
        "SET100_BREADTH = SET100_BREADTH.drop_duplicates(subset='Date', keep='first')\n",
        "SET100_BREADTH = SET100_BREADTH.sort_values(by='Date', ascending=True)\n",
        "SET100_BREADTH['Date'] = SET100_BREADTH['Date'].dt.date\n",
        "\n",
        "SET100_BREADTH_googleSheet = file.open(\"Thailand Market Breadth\").worksheet(\"SET100_BREADTH\")\n",
        "SET100_BREADTH_googleSheet.clear()\n",
        "gd.set_with_dataframe(SET100_BREADTH_googleSheet, SET100_BREADTH)\n",
        "\n",
        "print('SET100 Breadth Finished')\n",
        "\n",
        "end_time = dt.datetime.now()\n",
        "print(end_time)\n",
        "execution_time = end_time - start_time\n",
        "execution_time_seconds = execution_time.total_seconds()\n",
        "print('Execution Time: ',execution_time_seconds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBQlnFVqzyVF"
      },
      "source": [
        "### **Relative Strength Ranking and %Off 52-Week High**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "bXDzwhEGIxR4"
      },
      "outputs": [],
      "source": [
        "# Filter out new stocks\n",
        "New_Stock = list(CLOSE.columns[CLOSE.isnull().sum() > 250])\n",
        "New_Stock.append('Date')\n",
        "\n",
        "# RS Ranking Prep. data\n",
        "RS_data_ticker = SET_C_price.copy().drop(New_Stock, axis=1).tail(311).reset_index(drop=True)\n",
        "\n",
        "# 60 days ranking\n",
        "ROC_60d = RS_data_ticker.pct_change(periods=60).tail(1).T.reset_index(); ROC_60d.columns = ['Ticker', 'ROC_60d']\n",
        "\n",
        "# 120 days ranking\n",
        "ROC_120d = RS_data_ticker.pct_change(periods=120).tail(1).T.reset_index(); ROC_120d.columns = ['Ticker', 'ROC_120d']\n",
        "\n",
        "# 180 days ranking\n",
        "ROC_180d = RS_data_ticker.pct_change(periods=180).tail(1).T.reset_index(); ROC_180d.columns = ['Ticker', 'ROC_180d']\n",
        "\n",
        "# 250 days ranking\n",
        "ROC_250d = RS_data_ticker.pct_change(periods=250).tail(1).T.reset_index(); ROC_250d.columns = ['Ticker', 'ROC_250d']\n",
        "\n",
        "# Join DataFrame\n",
        "data_frames = [ROC_60d, ROC_120d, ROC_180d, ROC_250d]\n",
        "Relative_Strength = reduce(lambda left,right: pd.merge(left,right,on=['Ticker'], how='inner'), data_frames)\n",
        "\n",
        "# Weighting RS\n",
        "Relative_Strength['Weight_RS'] = 0.4*Relative_Strength.ROC_60d + 0.2*Relative_Strength.ROC_120d + 0.2*Relative_Strength.ROC_180d + 0.2*Relative_Strength.ROC_250d\n",
        "Relative_Strength['RS_Percentile_Rank'] = round((Relative_Strength.Weight_RS.rank(pct=True)) * 100, 2)\n",
        "Relative_Strength = Relative_Strength.sort_values(by=['RS_Percentile_Rank'], ascending=False)\n",
        "\n",
        "Relative_Strength.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "Relative_Strength.dropna(inplace=True)\n",
        "\n",
        "RS_Ticker = sorted(list(Relative_Strength.Ticker))\n",
        "RS_data_ticker = SET_C_price.copy().filter(RS_Ticker)\n",
        "\n",
        "# 60 days ranking\n",
        "ROC_60d = RS_data_ticker.pct_change(periods=60).tail(1).T.reset_index(); ROC_60d.columns = ['Ticker', 'ROC_60d']\n",
        "\n",
        "# 120 days ranking\n",
        "ROC_120d = RS_data_ticker.pct_change(periods=120).tail(1).T.reset_index(); ROC_120d.columns = ['Ticker', 'ROC_120d']\n",
        "\n",
        "# 180 days ranking\n",
        "ROC_180d = RS_data_ticker.pct_change(periods=180).tail(1).T.reset_index(); ROC_180d.columns = ['Ticker', 'ROC_180d']\n",
        "\n",
        "# 250 days ranking\n",
        "ROC_250d = RS_data_ticker.pct_change(periods=250).tail(1).T.reset_index(); ROC_250d.columns = ['Ticker', 'ROC_250d']\n",
        "\n",
        "# Join DataFrame\n",
        "data_frames = [ROC_60d, ROC_120d, ROC_180d, ROC_250d]\n",
        "Relative_Strength = reduce(lambda left,right: pd.merge(left,right,on=['Ticker'], how='inner'), data_frames)\n",
        "\n",
        "# Weighting RS\n",
        "Relative_Strength['Weight_RS'] = 0.4*Relative_Strength.ROC_60d + 0.2*Relative_Strength.ROC_120d + 0.2*Relative_Strength.ROC_180d + 0.2*Relative_Strength.ROC_250d\n",
        "Relative_Strength['RS_Percentile_Rank'] = round((Relative_Strength.Weight_RS.rank(pct=True)) * 100, 2)\n",
        "\n",
        "scale_min = 1\n",
        "scale_max = 99\n",
        "min = Relative_Strength['RS_Percentile_Rank'].min()\n",
        "max = Relative_Strength['RS_Percentile_Rank'].max()\n",
        "\n",
        "Relative_Strength['RS_Rating'] = round(((scale_max-scale_min)*((Relative_Strength['RS_Percentile_Rank'] - min)/(max - min))) + scale_min, 2)\n",
        "Relative_Strength = Relative_Strength.sort_values(by=['RS_Rating'], ascending=False)\n",
        "Relative_Strength = Relative_Strength[['Ticker', 'RS_Rating']].query(\"RS_Rating > 80\").reset_index(drop=True)\n",
        "\n",
        "RS_Ticker = sorted(list(Relative_Strength.Ticker))\n",
        "RS_H_Price = SET_H_price.copy().filter(RS_Ticker).tail(270).reset_index(drop=True)\n",
        "\n",
        "currentHigh = RS_H_Price.tail(1).reset_index(drop=True)\n",
        "High52Wk = RS_H_Price.rolling(250).max().tail(1).reset_index(drop=True)\n",
        "pct_Off52Wk = (currentHigh/High52Wk).T.reset_index()\n",
        "pct_Off52Wk = pct_Off52Wk.rename(columns={'index':'Ticker', 0:'%Off 52 Weeks High'})\n",
        "pct_Off52Wk['%Off 52 Weeks High'] = 100*pct_Off52Wk['%Off 52 Weeks High']\n",
        "\n",
        "SET_OFF52WK = round(Relative_Strength.merge(pct_Off52Wk, on='Ticker', how='inner'),2)\n",
        "SET_OFF52WK = SET_OFF52WK[(SET_OFF52WK['RS_Rating'] > 80)&(SET_OFF52WK['%Off 52 Weeks High'] > 80)]\n",
        "\n",
        "SET_OFF52WK_googleSheet = file.open(\"Thailand Market Breadth\").worksheet(\"SET_OFF52WK\")\n",
        "SET_OFF52WK_googleSheet.clear()\n",
        "gd.set_with_dataframe(SET_OFF52WK_googleSheet, SET_OFF52WK)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqQ9rO0iz5Tl"
      },
      "source": [
        "### **EPS Rating and SMR Ranking**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Q9ZW1TtTw7sZ"
      },
      "outputs": [],
      "source": [
        "weekday = (dt.date.today() + dt.timedelta(hours=7)).weekday()\n",
        "\n",
        "if weekday > 4: # Saturday and Sunday\n",
        "  smr_eps_rating = dict()\n",
        "  for i in symbols:\n",
        "    try:\n",
        "      stock = Stock(i)\n",
        "      try:\n",
        "        # SMR Rating: Sale Growth, Net Profit Margin, Return on Equity, and EBIT Margin\n",
        "        revenue_quarterly = stock.quarter_dataframe.sort_index(ascending = False).reset_index()[['Time', 'Revenue']]\n",
        "        QoQ_1 = (revenue_quarterly['Revenue'][0] - revenue_quarterly['Revenue'][4])/abs(revenue_quarterly['Revenue'][4] + 0.001)\n",
        "        QoQ_2 = (revenue_quarterly['Revenue'][1] - revenue_quarterly['Revenue'][5])/abs(revenue_quarterly['Revenue'][5] + 0.001)\n",
        "        QoQ_3 = (revenue_quarterly['Revenue'][2] - revenue_quarterly['Revenue'][6])/abs(revenue_quarterly['Revenue'][6] + 0.001)\n",
        "\n",
        "        saleGrowth_3Q = (QoQ_1 + QoQ_2 + QoQ_3)/3\n",
        "        netProfitMargin = stock.quarter_dataframe.NPM.sort_index(ascending=False)[0]\n",
        "        ROE = stock.yearly_dataframe.ROE.sort_index(ascending=False)[0]\n",
        "        \n",
        "        EBITDA = np.array(stock.yearly_dataframe.EbitDATTM.sort_index(ascending=False))[0]\n",
        "        DA = np.array(stock.yearly_dataframe.DA.sort_index(ascending=False))[0]\n",
        "        Revenue = stock.yearly_dataframe.Revenue.sort_index(ascending=False)[0]\n",
        "        ebitMargin = (EBITDA - DA)/(Revenue + 0.001)\n",
        "\n",
        "        # EPS Rank: %QoQ Earning per Share, Average %YoY Earning Growth, and Earning Stability Factor\n",
        "        eps_quarterly = stock.quarter_dataframe.sort_index(ascending = False).reset_index()[['Time', 'EarningPerShare']]\n",
        "        eps_annually = stock.yearly_dataframe.sort_index(ascending = False).reset_index()[['Fiscal', 'EarningPerShare']]\n",
        "\n",
        "        YoY_1 = (eps_annually['EarningPerShare'][0] - eps_annually['EarningPerShare'][1])/abs(eps_annually['EarningPerShare'][1] + 0.00001)\n",
        "        YoY_2 = (eps_annually['EarningPerShare'][1] - eps_annually['EarningPerShare'][2])/abs(eps_annually['EarningPerShare'][2] + 0.00001)\n",
        "        YoY_3 = (eps_annually['EarningPerShare'][2] - eps_annually['EarningPerShare'][3])/abs(eps_annually['EarningPerShare'][3] + 0.00001)\n",
        "\n",
        "        arr = np.array(eps_quarterly['EarningPerShare'][0:12])\n",
        "        norm = (arr-np.min(arr))/(np.max(arr)-np.min(arr))\n",
        "\n",
        "        rule_1 = (eps_quarterly['EarningPerShare'][0] - eps_quarterly['EarningPerShare'][4])/abs(eps_quarterly['EarningPerShare'][4] + 0.00001)\n",
        "        rule_2 = (eps_quarterly['EarningPerShare'][1] - eps_quarterly['EarningPerShare'][5])/abs(eps_quarterly['EarningPerShare'][5] + 0.00001)\n",
        "        rule_3 = (YoY_1 + YoY_2 + YoY_3)/3\n",
        "        rule_4 = np.std(norm)\n",
        "\n",
        "        smr_eps_rating[i] = [saleGrowth_3Q, netProfitMargin, ROE, ebitMargin, rule_1, rule_2, rule_3, rule_4]\n",
        "      except:\n",
        "        pass\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "  ranking = pd.DataFrame(smr_eps_rating).T\n",
        "  ranking.columns = ['saleGrowth', 'NPM', 'ROE', 'ebitMargin', 'Rule_1', 'Rule_2', 'Rule_3', 'Rule_4']\n",
        "\n",
        "  rating = pd.DataFrame()\n",
        "\n",
        "  # SMR Rating\n",
        "  factor_1 = 0.30  # Average % YoY Sale Growth \n",
        "  factor_2 = 0.30  # % Net Profit Margin\n",
        "  factor_3 = 0.20  # ROE\n",
        "  factor_4 = 0.20  # % EBIT Margin\n",
        "\n",
        "  rating['saleGrowth'] = factor_1*100*ranking['saleGrowth'].rank(pct=True).fillna(value=0, method=None)\n",
        "  rating['NPM'] = factor_2*100*ranking['NPM'].rank(pct=True).fillna(value=0, method=None)\n",
        "  rating['ROE'] = factor_3*100*ranking['ROE'].rank(pct=True).fillna(value=0, method=None)\n",
        "  rating['ebitMargin'] = factor_4*100*ranking['ebitMargin'].rank(pct=True).fillna(value=0, method=None)\n",
        "\n",
        "  rating['sumSMR'] = rating[['saleGrowth', 'NPM', 'ROE', 'ebitMargin']].agg('sum', axis=1)\n",
        "\n",
        "  # EPS Rank\n",
        "  factor_5 = 0.30   # % QoQ EPS Growth - Lastest Quarter\n",
        "  factor_6 = 0.30   # % QoQ EPS Growth - Prior Quarter\n",
        "  factor_7 = 0.30   # 3-yr Average % EPS Growth\n",
        "  factor_8 = 0.10   # Earning Stability Factor\n",
        "  rating['Rule_1'] = factor_5*100*ranking['Rule_1'].rank(pct=True).fillna(value=0, method=None)\n",
        "  rating['Rule_2'] = factor_6*100*ranking['Rule_2'].rank(pct=True).fillna(value=0, method=None)\n",
        "  rating['Rule_3'] = factor_7*100*ranking['Rule_3'].rank(pct=True).fillna(value=0, method=None)\n",
        "  rating['Rule_4'] = factor_8*100*(1/ranking['Rule_4']).rank(pct=True).fillna(value=0, method=None)\n",
        "\n",
        "  rating['sumEPS'] = rating[['Rule_1', 'Rule_2', 'Rule_3', 'Rule_4']].agg('sum', axis=1)\n",
        "\n",
        "  # Ranking\n",
        "  scale_min = 1\n",
        "  scale_max = 99\n",
        "\n",
        "  min_SMR = rating['sumSMR'].min()\n",
        "  max_SMR = rating['sumSMR'].max()\n",
        "\n",
        "  min_EPS = rating['sumEPS'].min()\n",
        "  max_EPS = rating['sumEPS'].max()\n",
        "\n",
        "  rating['EPS Rating'] = round(((scale_max-scale_min)*((rating['sumEPS'] - min_EPS)/(max_EPS - min_EPS))) + scale_min, 2)\n",
        "  rating['SMR Rating'] = round(((scale_max-scale_min)*((rating['sumSMR'] - min_SMR)/(max_SMR - min_SMR))) + scale_min, 2)\n",
        "\n",
        "  rating = rating.reset_index()\n",
        "  rating = rating[['index', 'EPS Rating', 'SMR Rating']]\n",
        "  rating.columns = ['Ticker', 'EPS Rating', 'SMR Rating']\n",
        "  rating = rating.sort_values(by=['SMR Rating'], ascending=False)\n",
        "\n",
        "  EPS_SMR = rating[(rating['EPS Rating'] > 70)&(rating['SMR Rating'] > 70)].reset_index(drop=True)\n",
        "  EPS_SMR_googleSheet = file.open(\"Thailand Market Breadth\").worksheet(\"SET_EPS_SMR\")\n",
        "  EPS_SMR_googleSheet.clear()\n",
        "  gd.set_with_dataframe(EPS_SMR_googleSheet, EPS_SMR)\n",
        "else:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJGuNGIOz-Ee"
      },
      "source": [
        "### **Mark Minervini Trend Template**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "f0p_OjQFydla"
      },
      "outputs": [],
      "source": [
        "RS_Ticker = sorted(list(Relative_Strength.Ticker))\n",
        "RS_H_Price = SET_H_price.copy().filter(RS_Ticker).tail(300).reset_index(drop=True)\n",
        "RS_L_Price = SET_L_price.copy().filter(RS_Ticker).tail(300).reset_index(drop=True)\n",
        "RS_C_Price = SET_C_price.copy().filter(RS_Ticker).tail(300).reset_index(drop=True)\n",
        "RS_Volume = SET_Volume.copy().filter(RS_Ticker).tail(300).reset_index(drop=True)\n",
        "\n",
        "high_52wk = RS_H_Price.rolling(window=250).max().tail(1)\n",
        "low_52wk = RS_C_Price.rolling(window=250).min().tail(1)\n",
        "\n",
        "filter_1 = RS_C_Price.copy()\n",
        "\n",
        "current_price = filter_1.tail(1)\n",
        "ma50 = filter_1.rolling(window=50).mean().tail(1)\n",
        "ma150 = filter_1.rolling(window=150).mean().tail(1)\n",
        "ma200 = filter_1.rolling(window=200).mean().tail(1)\n",
        "\n",
        "filter_1_cond_1 = (current_price > ma150).T.reset_index(); filter_1_cond_1.columns = ['Ticker', 'Cond_1']\n",
        "filter_1_cond_1 = filter_1_cond_1[filter_1_cond_1['Cond_1'] == True]\n",
        "filter_1_cond_2 = (current_price > ma200).T.reset_index(); filter_1_cond_2.columns = ['Ticker', 'Cond_2']\n",
        "filter_1_cond_2 = filter_1_cond_2[filter_1_cond_2['Cond_2'] == True]\n",
        "filter_1 = list(filter_1_cond_1.merge(filter_1_cond_2, on='Ticker', how='inner')['Ticker'])\n",
        "\n",
        "filter_2 = (ma150 > ma200).T.reset_index(); filter_2.columns = ['Ticker', 'Cond_1']\n",
        "filter_2 = list(filter_2[filter_2['Cond_1'] == True]['Ticker'])\n",
        "\n",
        "filter_3 = RS_C_Price.copy()\n",
        "filter_3_ma200 = filter_3.rolling(window=200).mean()\n",
        "filter_3_ma200 = filter_3_ma200.dropna()\n",
        "\n",
        "slope_20 = pd.DataFrame((filter_3_ma200.iloc[-1] - filter_3_ma200.iloc[-20])/20).reset_index()\n",
        "slope_20.columns = ['Ticker', 'slope_20']; slope_20['degree_20d'] = np.arctan(slope_20['slope_20'])*(180/math.pi)\n",
        "\n",
        "slope_40 = pd.DataFrame((filter_3_ma200.iloc[-1] - filter_3_ma200.iloc[-40])/40).reset_index()\n",
        "slope_40.columns = ['Ticker', 'slope_40']; slope_40['degree_40d'] = np.arctan(slope_40['slope_40'])*(180/math.pi)\n",
        "\n",
        "slope_60 = pd.DataFrame((filter_3_ma200.iloc[-1] - filter_3_ma200.iloc[-60])/60).reset_index()\n",
        "slope_60.columns = ['Ticker', 'slope_60']; slope_60['degree_60d'] = np.arctan(slope_60['slope_60'])*(180/math.pi)\n",
        "\n",
        "slope_80 = pd.DataFrame((filter_3_ma200.iloc[-1] - filter_3_ma200.iloc[-80])/80).reset_index()\n",
        "slope_80.columns = ['Ticker', 'slope_80']; slope_80['degree_80d'] = np.arctan(slope_80['slope_80'])*(180/math.pi)\n",
        "\n",
        "slope_100 = pd.DataFrame((filter_3_ma200.iloc[-1] - filter_3_ma200.iloc[-100])/100).reset_index()\n",
        "slope_100.columns = ['Ticker', 'slope_100']; slope_100['degree_100d'] = np.arctan(slope_100['slope_100'])*(180/math.pi)\n",
        "\n",
        "data_frames = [slope_20, slope_40, slope_60, slope_80, slope_100]\n",
        "\n",
        "slope = reduce(lambda left,right: pd.merge(left,right,on=['Ticker'], how='inner'), data_frames)\n",
        "slope = slope[['Ticker', 'degree_20d', 'degree_40d', 'degree_60d', 'degree_80d', 'degree_100d']]\n",
        "filter_3 = list(slope[(slope['degree_20d'] > 5)&(slope['degree_40d'] > 4)&(slope['degree_60d'] > 3)&(slope['degree_80d'] > 2)&(slope['degree_100d'] > 1)]['Ticker'])\n",
        "\n",
        "filter_4 = ((ma50 > ma150)&(ma50 > ma200)).T.reset_index(); filter_4.columns = ['Ticker', 'Cond_1']\n",
        "filter_4 = list(filter_4[filter_4['Cond_1'] == True]['Ticker'])\n",
        "\n",
        "filter_5 = (current_price > ma50).T.reset_index(); filter_5.columns = ['Ticker', 'Cond_1']\n",
        "filter_5 = list(filter_5[filter_5['Cond_1'] == True]['Ticker'])\n",
        "\n",
        "filter_6 = (current_price > 1.3*low_52wk).T.reset_index(); filter_6.columns = ['Ticker', 'Cond_1']\n",
        "filter_6 = list(filter_6[filter_6['Cond_1'] == True]['Ticker'])\n",
        "\n",
        "filter_7 = (current_price > 0.75*high_52wk).T.reset_index(); filter_7.columns = ['Ticker', 'Cond_1']\n",
        "filter_7 = list(filter_7[filter_7['Cond_1'] == True]['Ticker'])\n",
        "\n",
        "MarkMinervini = sorted(list(set(filter_1) & set(filter_2) & set(filter_3) & set(filter_4) & set(filter_5) & set(filter_6) & set(filter_7)))\n",
        "\n",
        "if len(MarkMinervini) > 0:\n",
        "  MM_Ticker = pd.DataFrame(MarkMinervini, columns=['Ticker'])\n",
        "  MarkMinervini = MM_Ticker.merge(Relative_Strength, on='Ticker', how='inner')\n",
        "else:\n",
        "  pass\n",
        "\n",
        "try:\n",
        "  MarkMinervini_googleSheet = file.open(\"Thailand Market Breadth\").worksheet(\"MARK_MINERVINI\")\n",
        "  MarkMinervini_googleSheet.clear()\n",
        "  gd.set_with_dataframe(MarkMinervini_googleSheet, MarkMinervini)\n",
        "except:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pittA2kT2KKQ"
      },
      "source": [
        "### **Summary Time**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "hSoAI32l1Vw3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2063c0ce-354a-4a60-93f2-a896bb375c41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total_Time:  214.163303\n"
          ]
        }
      ],
      "source": [
        "Totally_Done = dt.datetime.now()\n",
        "Total_Time = (Totally_Done - Actually_Starting).total_seconds()\n",
        "print('Total_Time: ', Total_Time)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "ralok5CB93ZR",
        "ZReWjW2jIlTG",
        "UzgvmoFqtiQi",
        "9UbEzyiQuF_P",
        "xurTs_OHuIUs"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyMwqUPBOiS1ULgSCN3Y3dRZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}